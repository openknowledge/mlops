<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=10.0, user-scalable=yes">

	<title>Workshop: MLOps d2d 2022</title>

	<link rel="stylesheet" href="revealjs/reveal.js/dist/reset.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/reveal.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/theme/white.css" />

    <!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/monokai.css"> -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/zenburn.css"> -->
    <link rel="stylesheet" href="revealjs/highlight-js-github-theme.css" />
    <link rel="stylesheet" href="revealjs/styles.css" />

</head>

<body style="background-color: whitesmoke;">
	<div class="reveal">
		<div class="slides">

<!-- 
M3 Workshop:

Titel: MLOps mit Python und TensorFlow
Untertitel: Machine Learning betrachtet als ein Engineering Problem

Level: Fortgeschritten

Eine praxistaugliche Anwendung mit Techniken des Machine Learnings zu entwickeln ist in erster Linie eine
Herausforderung im Bereich des Engineerings. Dabei geht es mehr um den
Entwicklungsprozess und weniger um die konkret eingesetzte Technik und
die Bibliotheken.

In diesem Workshop gehen wir gemeinsam durch die unterschiedlichen Phasen eines Machine Learning Projekts, von der
Exploration und Validierung eines Machine Learning Ansatzes √ºber
die Professionalisierung zu einem stabilen St√ºck Software bis in den produktiven Einsatz. 

In der ersten Phase entwickeln wir eine Machine Learning basierte L√∂sung f√ºr ein gegebenes Problem.
Dabei iterieren wir mit der Hilfe von Notebooks schnell durch unterschiedliche Experimente. Am Ende dieser Phase haben
wir unsere Idee (hoffentlich) validiert und k√∂nnen
mit diesem Ansatz in die n√§chste Phase √ºbergehen.

In Phase zwei professionalisieren wir unseren gefundenen Ansatz in Richtung Produktion. Dabei bringen wir unsere Experimente in 
Python Module und bereiten diese f√ºr die Produktion vor. In dieser letzten Phase der Produktion sehen wir uns an, wie man ein solches Modell betreiben und monitoren kann.

Alle Schritte sind hands-on und wir werden genug Zeit f√ºr Diskussionen haben. 

Vorkenntnisse

Teilnehmer sollten entweder mit den Werkzeugen und dem Vorgehen im Bereich Data Science und/oder als Machine Learning
Engineer grundlegende Erfahrung haben. Die Sprache Python ist ebenso Grundlage.
Die Werkzeuge sind von untergeordneter Bedeutung. Kenntnisse im Bereich TensorFlow oder Scikit-Learn, Jupyter Notebooks
und Colab erleichtern die Entwicklung jedoch.

Lernziel

In diesem durch praktische √úbungen gepr√§gten Workshop wollen wir zusammen die Herausforderungen kennen lernen und
meistern, die sich aus dem Ziel "langfristig in Produktion" sein ergeben.


Kurzbeschreibung f√ºr M3

Eine praxistaugliche Anwendung mit Techniken des Machine Learnings zu entwickeln ist in erster Linie eine
Herausforderung im Bereich des Engineerings.

In diesem Workshop gehen wir gemeinsam durch die unterschiedlichen Phasen eines Machine Learning Projekts, von der
Exploration und Validierung eines Machine Learning Ansatzes √ºber
die Professionalisierung zu einem stabilen St√ºck Software bis in den produktiven Einsatz. Dabei geht es mehr um den
Entwicklungsprozess und weniger um die konkret eingesetzte Technik und
die Bibliotheken.

Vorbereitung

Dieser Workshop geht davon aus, dass du bereits eine Entwicklungsumgebung mit IDE, Python Distribution und Git auf eurem Rechner lauff√§hig hast. 
Sollte das nicht der Fall sein und du nicht sicher bist, welche Software passt, installiere bitte:
1. Anaconda: https://www.anaconda.com/products/individual
2. Visual Studio Code: https://code.visualstudio.com/
3. Git: https://git-scm.com/downloads
4. Docker: https://docs.docker.com/get-docker/ und https://docs.docker.com/compose/install/

Wir werden auf einem Beispielprojekt arbeiten. Habt bitte bereits vor dem Workshop
1. Das Projekt geklont: https://github.com/DJCordhose/insurance-ml
2. Das Projekt wie im Readme beschrieben entweder lokal oder/und √ºber Docker installiert


Agenda

09:00 Uhr: Beginn

Einf√ºhrung in das Projekt und √úberblick
* Grundlagen von ML
* Phasen eines ML Projekts
* Unser Beispiel erkunden
* TensorFlow und Scikit-Learn

10:45 - 11:00 Uhr: Kaffeepause

Phase I: Exploration
* Was ist der Sinn dieser Phase, was sind die Ziele
* Wieso bauen wir hier keine "richtige" Software?
* Arbeit mit Notebooks und Colab
* √úbergang zu Phase II 

12:30 - 13:30 Uhr: Mittagspause

Phase II: Professionalisierung
* Vom Notebook zum Modul
* Vom Notebook zum Script
* Testing mit Python Unit testing
* Finden von Fehlern und Debugging
* Was muss man dokumentieren?
* Arbeit in der IDE
* √úbergang zu Phase III oder I

15:00 - 15:15 Uhr: Kaffeepause

Phase III: Produktion
* Produktivsetzung
* Drifts, Monitoring, Alerting
* Evidently, Prometheus, Grafana 
* √úbergang zu Phase II oder I

ca. 17:00 Uhr: Ende


---

Data2Day
https://www.data2day.de/cfp.php

Workshop
https://www.data2day.de/veranstaltung-15183-0-mlops-mit-python.html

9:00 - 16:00

MLOps mit Python

Viele Unternehmen haben die Phase des reinen Experimentierens mit Machine Learning hinter sich gelassen und fragen sich
nun, wie man ein Machine Learning Modell so betreibt wie ein traditionelles St√ºck Software. Also wie man es in Produktion bringt und dort
erfolgreich h√§lt. MLOps ist der Name f√ºr diesen Ansatz. 

In diesem Workshop werden wir uns anhand eines durchg√§ngigen Beispiel durch die Phasen eines Machine Learning Projekts bewegen:
- Exploration: Entwicklung eines Modells mit Jupyter Notebooks und TensorFlow
- Professionalisierung: Refactoring in Module
- Produktion und Monitoring: Produktivsetzung mit Flask und Docker, Monitoring mit Prometheus, Evidently und Grafana

Als ML Werkzeug werden wir TensorFlow nutzen, allerdings ist dies nur ein Beispiel und alles gezeigte gilt auch f√ºr alle
anderen Frameworks. Es wird lediglich Erfahrung mit Python und ein Verst√§ndnis f√ºr ein Machine Learning Projekt
vorausgesetzt. Alles andere wird - soweit n√∂tig - im Workshop eingef√ºhrt.

Lernziel:
Teilnehmer sollen anhand eines in sich stimmigen Satzes von Werkzeugen die Konzepte und Ans√§tze im Bereich MLOps kennen lernen und diskutieren k√∂nnen.

---

Talk: 45 Minuten

https://www.data2day.de/veranstaltung-15048-0-monitoring-von-drift-mit-prometheus-grafana-und-evidently.html

Monitoring von Drift mit Prometheus, Grafana und Evidently

Machine-Learning-Modelle erfordern besondere Ma√ünahmen beim Monitoring. Die Vorhersagekraft des Modells ist dabei
besonders wichtig, aber oft nicht direkt beobachtbar.

In diesem Live-Demo ohne Folien sehen wir uns eine Machine Learning Anwendung an, die mit einem bestimmten Modell in
Produktion gegangen ist. Das Model basiert auf TensorFlow und wir mit einem Flask √ºber Docker serviert. In simulierten
Anfragen detektieren wir einen Drift, der einen Alarm in Grafana ausl√∂st. Diesen interpretieren wir und entscheiden, ob
er kritisch und was die passende Ma√ünahme ist.


Die Teilnehmenden bekommen einen Eindruck von einem realistischen Setup f√ºr das Monitoring von Drift. Es wird erkl√§rt,
warum das so wichtig ist und wie man eine passende Ma√ünahme ableitet

---

Aus: https://arxiv.org/abs/2007.06299
The machine learning lifecycle extends beyond the deployment stage. Monitoring deployed models is crucial for 
continued provision of high quality machine learning enabled services. 
Key areas include model performance and data monitoring, detecting outliers and data drift using statistical techniques, 
and providing explanations of historic predictions. We discuss the challenges to successful implementation of solutions 
in each of these areas with some recent examples of production ready solutions using open source tools.


---

OOP

Titel: MLOps - wie bringt man ein Machine Learning Modell in Produktion und h√§lt es dort?

MLOps besch√§ftigt sich mit dem Thema, ein Machine Learning Modell in Produktion zu bringen und es dort erfolgreich zu betreiben.

Ein Machine Learning Modell l√§uft fast immer als Teil eines komplexeren Software Systems. 
Und so ist die die erste Herausforderung ist, das vermeintlich fertige Modell in in ein solches System zu integrieren.
F√ºr die Produktion m√ºssen Metriken erarbeitet werden, die die Performanz des Modells auch in Zukunft bewerten k√∂nnen.
Da es h√§ufig (zumindest zeitnah) keine gesicherten Erkenntnisse √ºber die Qualit√§t der Vorhersage gibt, m√ºssen 
Platzhalter daf√ºr gefunden werden.
Ebenso muss √ºberlegt werden, wie man die Ursache f√ºr die Verschlechterung der Vorhersagequalit√§t eigentlich gemacht werden sollte und wie man 

die f√ºr einzelne Beispiele 

Sobald das Modell in Produktion ist, muss es auf eine 




https://en.wikipedia.org/wiki/MLOps


---

https://qconsf.com/track/oct2022/mlops



Talk: Drift Detection in MLOps

MLOps deals with the topic of bringing a machine learning model into production and keeping it there.



problem of detecting and handling drift in machine learning models.

Bringing a machine learning model into production and making it stay there is a complex task.

Once your model is in production you need to monitor it for degrading performance. 
Since many applications do not provide immediate feedback (or none at all) on the prediction quality, 

you need to provide a way to

you need to monitor it for drift.

In this talk I will look at existing tools and approaches independent of any solution provider from the perspective of a practitioner.


In this talk we will look at https://evidentlyai.com/ and https://github.com/SeldonIO/alibi-detect

Once a drift has been detected, we will use the alibi-detect library to provide a detailed explanation of the drift.

Eventually, we will act on the explanation and provide a solution based on serverity and the context of the drift.


Oliver Zeigermann is Head Of AI at Open Knowledge, a leading provider of machine learning services. 
He is an O‚ÄôReilly book author and the author of a Manning video course. Oliver has been a speaker at various conferences 
including ODSC West, TensorFlow world, and MLConf. 
His main interest is in the intersection between traditional software engineering and machine learning.

What would be the 3-4 main actionable takeaways you would like your audience to remember and to use after your talk?
1. DevOps for machine learning has special requirements
2. Drift of various types is a problem not only of machine learning, but all heuristic business code
3. Detecting, evaluating and acting upon drift is the most critical part of the machine learning lifecycle 

-->
 

<!-- <section data-markdown class="local preparation hide">
* Serving Teil aus Amd und abcd am Ende einf√ºgen?
* Details
  * √úbersicht Zusammenspiel Prometheus, Grafana etc.
  * Zuerst die gleichen Daten gegen den Server schicken, um zu sehen, dass kein Drift
  * P Wert anders einstellen
  * Gucken: Was kann ich aus dem Talk noch √ºbernehmen?
	
* Lars alle √úbungen zeigen
* Debugging des Tests einmal durchspielen	
* Sicher stellen: beenden mit grafischer √úbung in der wir noch ordentlich unterst√ºtzen k√∂nnen und das ganze mit Kontakt beenden
* Phase 3 k√∂nnte den ganzen Nachmittag einnehmen, wir haben eher zu viel Material

</section>
 -->
<!-- <section data-markdown class="todo">

</section> -->

<!-- <section data-markdown class="todo">

https://developers.google.com/machine-learning/guides/rules-of-ml		
Hidden Technical Debt in Machine Learning Systems: https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf	
</section>


<section data-markdown class="todo">
### The world is messy
		
	Statisticians that believe that doing the statistics is the hard part reveal they have no sense how messy real world data collection is.

Hardest part is having a dataset that reflects reality. Time delays, simple dismissals, incentives, clerical errors, term ambiguity, on and on
(https://twitter.com/normonics/status/1511021293969235975?t=spHs38A9j38gdGASJMt1AA&s=03) 
</section> -->

<section data-markdown class="todo">
	<textarea data-template>
### ML Agile

* ML Prozess beschreiben
* Wieso passt das auf allgemeines Prozess

Business getriebene Software Entwicklung

Wie mir Business Metrik dabei helfen bessere Software zu bauen

Business Metrik Umsatz ist zu grob

Klassisches Dev hat Problem
- R√ºckkopplung und Dev vs Business

Tag Team
- Der ML Typ 

Es gibt Teilbereiche der Software-Entwicklung, in denen diese Probleme gar nicht erst auftreten. Was k√∂nnen wir von diesen Bereichen lernen. 

https://www.impactmapping.org/
https://www.impactmapping.org/example.html

marty cagan inspired / empowered
</textarea>
</section>


<section data-markdown class="todo">
	<textarea data-template>
### Feature Engineering

* Geburtsdatum in Alter
* Automarke hat keinen Einfluss (evtl nicht realistisch)
  * Autotyp Sportwagen, Familienkutsche
  * oder korrelation

https://www.kaggle.com/datasets/buntyshah/auto-insurance-claims-data
https://www.kaggle.com/datasets/xiaomengsun/car-insurance-claim-data
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Talk herausziehen

Idee: eine Grafik bleibt eine Grafik, aber wie geht das nun in echt? 
	
Story
1. Wir haben Modell
2. Professionalisieren
3. In Prod
4. Log gegen Metrik Server
4b. Wir brauchen einen Proxy f√ºr Metrik, da wir zeitnah keine Ground Truth bekommen 
5. Prometheus und Grafana zeigen Entwicklung 
6. Drift ist detectiert √ºber Evidently
7. Alibi Explain und Detect helfen bei der Interpretation / Analyse 
8. Wir untersuchen, was man sinnvoll tun kann. M√ºssen wir Weltwissen einbringen?
9. Wenn man sein Model nicht versteht und es in Prod driftet, was macht man denn dann √ºberhaupt?
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Installation

* Git / Github
* Docker
	</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Werkzeuge

* CI/CD Github Actions
* Versionierung der Daten √ºber Git Lfs / DVC
* Deployment des Modell-Artefakts √ºber was? Artefactory? Docker Registry?
* Flask
* Evidently
* Prometheus
* Alert Manager
* Grafana

	</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
# Folgende Folien zum Thema Granularit√§t einbauen
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/mlops/mlops-systems.png'>					

<small>

https://twitter.com/nkoumchatzky/status/1525904101619417095
</small>

    </textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Spezielle Anforderungen an die Entwicklung von ML Anwendungen

* Modelle so z√§hmen / trainieren, dass sie √ºberhaupt funktionieren
* Debuggen
* In-/Output kodieren
* Fallback f√ºr invalide Input- oder Output-Bereiche
* Automatisierte Tests

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Ein gro√ües Modell oder (mehrere) kleinere

<img src="img/mlops/Micro-ML.png">
</textarea>
</section>


<section data-markdown class="todo">
	<textarea data-template>
### Prometheus@Soundcloud, Antritts-Blogpost
https://developers.soundcloud.com/blog/prometheus-monitoring-at-soundcloud
</textarea>
</section>


<section data-markdown class="todo">
	<textarea data-template>
### Alert Manager von Prometheus nutzen

https://github.com/prometheus/alertmanager
</textarea>
</section>


<section data-markdown class="todo">
	<textarea data-template>

Emeli Dral (@EmeliDral) twitterte um 7:52 AM on Do., Juni 09, 2022:
Finally published the ML monitoring tutorial I gave at the Stanford CS 329S course!

üìö How to set up ML monitoring
üìä How to build dashboards
üèó How to automate the checks

Blog: https://t.co/8Y4grc7koo

Video: https://t.co/gIkkOnFISL

Thanks to @chipro for inviting me! https://t.co/jDt5crDxwo
(https://twitter.com/EmeliDral/status/1534775296372658176?t=xbxaT1E7Aw3lK8Lg1x3-oQ&s=03) 
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>

Shreya Shankar (@sh_reya) twitterte um 3:29 AM on Mi., Juni 22, 2022:
Honestly: sometimes I feel defeated because ML observability is so hard. All facets are hard -- detecting, diagnosing, reacting to bugs. We don't have realtime ground truth labels (except recsys) so we don't know asap when performance goes down. Lots of $$ left on the table (1/6)
(https://twitter.com/sh_reya/status/1539420163480489984?t=mk7_0Zu_FwFMag24QHHjSg&s=03) 
</textarea>
</section>


<section data-markdown class="todo">
	<textarea data-template>
## Statistik		
### Checken, ob was spannendes dabei ist
https://www.isi-web.org/events
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Prometheus / Grafana

https://github.com/trion-development/microservice-summit-monitoring		
https://github.com/trion-development/workshop-monitoring-prometheus-grafana

* Alerts eher in Prometheus defininieren und als Time Series in Grafana eher anzeigen?
  * https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  * https://prometheus.io/docs/practices/alerting/
* Erzeugt automatisch Metriken wie Zeit und Exception bei Spring Boot App: https://micrometer.io
* https://grafana.com/grafana/plugins/
* https://grafana.com/grafana/dashboards/
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Vorgehen

* Konzentration auf das eigentliche Prod Problem
* Mit fertigem Modell und herausgezogenen Libs und Scripten anfangen
* Block 1: Installation, Stand der Entwicklung, Intro MLOps
* 

Offen
* CI/CD
  * Wie binden wir Scripte ein?
* Daten? DVC?
* Feature Store?
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
- Links auf D2D setzen
- Installation als Teil des Programms
  - Inkl. Anaconda und git, etc.
  
- Script besser ber√ºcksichtigen
  - Scripts in Ordner herausziehen  
  - Script und Libs in √úbersichtsgrafik
  - Training Script als d√ºnnen Wrapper um train.py
  
Didaktik:
- Co-Trainer anwerben

Tests
- Tests √ºber pytest von der Kommandozeile
	</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Findings

- Installation kann nicht vorausgesetzt werden
- Herausziehen von libs: wissen nicht was man herausziehen soll
- Keiner kennt die Logging Tools
- Stats sind kompliziert
- Versionen Pinnen in Phase II
  - conda env export > environment. yml

- Gemeinsam nochmal durch das Notebook: was kann raus? Jeder w√§hlt eine Funktion aus und zieht sie raus. Ich mache eine vor
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Evidently und Stats
- zeigen dass mit Originaldaten kein Drift (daf√ºr nat√ºrlich erstmal daf√ºr sorgen, dass es so ist)
- Fenster Breite anpassen 
- Andere Metrik?
  * Wasserstein ist schon gut: CNN: Wasserstein ist ne echte Metrik, kl nicht. Man kann also echte topologische Unterschiede betrachtet. Wie zb f√ºr visuelle Sachen.
  * https://stats.stackexchange.com/questions/295617/what-is-the-advantages-of-wasserstein-metric-compared-to-kullback-leibler-diverg 
- andere p-value

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Evidently in (eigenem?) Notebook nutzen

Verteilung der Features bei Normal und Drift In Scatter Plot mit ausgeben und Plot in Folien zeigen
</textarea>
</section>


<section data-markdown class="todo">
	<textarea data-template>

Evaluation is one of the most important aspects of ML but today‚Äôs evaluation landscape is scattered and undocumented which makes evaluation unnecessarily hard.

For that reason we are excited to release ü§ó Evaluate!

https://t.co/x9DMvdjCMi

Let‚Äôs take a tour: https://t.co/wLuHZ6Ict1
(https://twitter.com/lvwerra/status/1531652407909920769?t=gPIcSQ2aB79Q2GEzLTpIMg&s=03) 

</textarea>
</section>


<section data-markdown class="todo">
	<textarea data-template>
### Directory Struktur

<img src="img/todo/matthias-directory-structure.jpg">

</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Testing

<img src="img/todo/matthias-testing.jpg">

</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Alternative Algorithmen und Decision Boundaries

<img src="img/insurance-new/dec_bound_adaboost.png">
<img src="img/insurance-new/dec_bound_rf.png">

</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
Fear of deploys is the largest source of technical debt and wasted time in most engineering orgs.
(https://twitter.com/mipsytipsy/status/1530664380961943553?t=tbi4R3SZwHMuwFl-0_4EGg&s=03) 
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Nothing is static

* Nothing is static, everything changes. But some things change faster than others. Culture changes slowly. Human nature is even slower.
(https://twitter.com/fchollet/status/1540548737302335490?t=FbGHMnaXfmZgKkARKIK2YA&s=03) 
* https://de.wikipedia.org/wiki/Panta_rhei
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Bevor es los geht

https://www.data2day.de/veranstaltung-15183-0-mlops-mit-python.html

1. WLAN: xxx
1. Diese Folien: https://bit.ly/data2day-2022-mlops
1. Falls noch nicht geschehen, bitte das Projekt wie in der Beschreibung (link oben) installieren
1. Nachher gibt es noch Zeit und es gibt auch einen Fallback

_Bei Problemen den Nachbar oder Tobias und Olli fragen_   
</textarea>
</section>

<!-- 
MLOps mit Python

https://www.data2day.de/veranstaltung-15183-0-mlops-mit-python.html

Agenda:

09:00 Uhr: Beginn

Einf√ºhrung
- MLOps: Phasen und Workflow in einem ML Projekt
- Daten entstehen √ºber die Zeit
- Unser Fallbeispiel und Datenbasis

10:45 - 11:00 Uhr: Kaffeepause

Ein lauff√§higes ML System
- Installation und Start des ML Systems
  - Docker
  - Flask
- wichtige Metriken
- Was kann man wie testen? 

12:30 - 13:30 Uhr: Mittagspause

Betrieb
* Benutzung des Modells simulieren
* Drifts, Monitoring, Alerting
* Evidently, Prometheus, Grafana
* Bewertung von Drift

15:00 - 15:15 Uhr: Kaffeepause

Abschluss / R√ºckblick
* Ausblick: was passiert nach Produktivsetzung?
* Offene Fragen

ca. 16:00 Uhr: Ende


TECHNISCHE ANFORDERUNGEN
Falls ihr ein Ger√§t eurer Firma verwendet, √ºberpr√ºft vorher bitte, ob eines der folgenden, gelegentlich vorkommenden Probleme bei euch auftreten k√∂nnte.
Workshop-Teilnehmer:in hat keine Administrator-Rechte.
Corporate Laptops mit √ºberm√§√üig penibler Sicherheitssoftware
Gesetzte Corporate-Proxies, √ºber die man in der Firma kommunizieren muss, die aber in einer anderen Umgebung entsprechend nicht erreicht werden.

Vorbereitung
Dieser Workshop geht davon aus, dass du bereits eine Python-Entwicklungsumgebung mit IDE, Python Distribution und Git 
auf deinem Rechner lauff√§hig hast.
Sollte das nicht der Fall sein und du nicht sicher bist, welche Software passt, installiere bitte:
* Anaconda: https://www.anaconda.com/products/individual
* Visual Studio Code: https://code.visualstudio.com/
* Git: https://git-scm.com/downloads
* Docker: https://docs.docker.com/get-docker/

Wir werden auf einem Beispielprojekt arbeiten. Hab bitte bereits vor dem Workshop
das Projekt https://github.com/openknowledge/mlops-data2day geklont und wie im Readme beschrieben installiert.
 -->

 <section data-markdown class="todo">
	<textarea data-template>
### MLOps @ Gitlab

* For traditional software, logic is made explicit through code, while on for ML the logic is implicit in the data, and extracted through a variety of technique's.
* Note that MLOps is not a branch of DevOps, it's a superset of it.


 https://gitlab.com/gitlab-org/incubation-engineering/mlops/meta/-/issues/58
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Story (auch f√ºr Talk mit Schwerpunkt auf hinteren Teil)

Orchestrierung hier h√§ndisch, es geht um das Konzept und welches Tool das richtige ist h√§ngt vom Projekt und der Umgebung ab.

1. Problemstellung: innovative Kfz-Versicherungsgesellschaft
1. Notebook Server Starten
1. Exploration durchgehen: http://localhost:8888/notebooks/notebooks/exploration.ipynb
   1. Features
   1. Was wollen wir vorhersagen?
1. √úberblick √ºber die 3 Phasen, wieso kann ich damit nicht in Prod gehen
1. Professionalisierung: Notebook in Libs und Scripte
1. Build: Scripte manuell laufen lassen
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./train.py -d ../data/reference.csv -m classifier`
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./validate.py -d ../data/reference.csv -m classifier`
   1. Modell in app Ordner schieben
   1. Diskussion: Modell und Daten sind gro√ü oder Binary, versionieren, aber nicht in Git
1. Produktion
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/app$ ./app.py`
   1. `http://localhost:8080/ping`
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day$ python -m http.server`
   1. `http://localhost:8000/app/client.html`
   1. in `app/client.html` Werte anpassen und herumspielen damit
1. Produktion in Docker
   1. Notwendig: Volumes von Prometheus und Grafana platt machen, um von 0 zu starten???
   1. `(base) olli@DESKTOP-BEN73DP:~/mlops-data2day$ docker compose up --build`
   1. `http://localhost:8085/metrics`
   1. `http://localhost:9090`
   1. `http://localhost:3000/`
1. Produktion simulieren
   1. Story:
      1. die Performance des Modells degradiert
	  1. aber wir haben erst nach Jahren eine Ground Truth, die uns das anhand der Metrik zeigt
	  1. wir simulieren 3 Jahre Betrieb mit
         1. Leute werden immer √Ñlter, das passiert aber langsam (age)
	     1. Es wird immer weniger Auto gefahren, Leute steigen um auf die Bahn und √∂ffentliche Verkehrsmittel (miles)
	     1. Die Sicherheit der Autos wird immer besser und der Einfluss der individuellen Fahrleistung wird verringert (emergency_breaking, pred) 
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day$ ./scripts/example_run_request.py` 
   1. `http://localhost:8085/metrics`
   1. `http://localhost:3000/d/U54hsxv7k/evidently-data-drift-dashboard?orgId=1&refresh=5s`
1. Analyse   
  1. Drift ist offensichtlich, aber gibt es √ºberhaupt ein Problem?
  1. http://localhost:8888/notebooks/notebooks/analysis.ipynb
  1. Letztlich bekommen wir GT rein von Daten, die 2 Jahre alt sind
  1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./validate.py -d ../data/month-12.csv -m classifier`
  1. Performance des Modells ist stark zur√ºck gegangen
  1. Wir sollten neu trainieren: zur√ºck zu Phase II 
  1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./train.py -d ../data/month-12.csv -m classifier`
  1. Neues Modell erf√ºllt nicht mehr unsere Anforderungen: zur√ºck zu Phase I
     1. Anforderungen √§ndern?
	 1. Modell tunen?
</textarea>
</section>

			<section data-markdown>
				<textarea data-template>
# MLOps mit Python         

data2day 2022, https://www.data2day.de/veranstaltung-15183-0-mlops-mit-python.html

Tobias Kurzydym / lars.roewekamp@openknowledge.de

Oliver Zeigermann / oliver.zeigermann@openknowledge.de


<!-- Folien: https://bit.ly/m3-2022-mlops -->
Folien: https://openknowledge.github.io/mlops-data2day/2022-d2d-workshop.html
    </textarea>
			</section>



<section data-markdown>
  <textarea data-template>
### Wer ist Tobias

<img src='img/Tobias_Kurzydym_3.jpg'>

<p>
<a target="_blank" href="mailto:tobias.kurzydym@openknowledge.de">Tobias Kurzydym</a>:
Dev
</p>    
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Olli

<div style="display: flex;">
<div style="flex: 50%;">
  <a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
  <img src='img/ml-buch-v2.jpg' height="400">
  </a>
</div>
<div style="flex: 50%; font-size: x-large;">
  <img src='img/olli-opa.jpeg'>
</div>
</div>
<p>
<a target="_blank" href="mailto:oliver.zeigermann@openknowledge.de">Oliver Zeigermann</a>:
Head of AI@OpenKnowledge
</p>    
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Wer seid ihr?

* Was macht ihr?
* Was wisst ihr schon?
* Warum seid ihr hier?
</textarea>
</section>

	<section data-markdown>
		<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauff√§higes ML System
* Betrieb
* Abschluss / R√ºckblick

</textarea>
	</section>

	<section data-markdown>
		<textarea data-template>
# Agenda

* _Grundlagen MLOps, Unser Beispiel_
* Ein lauff√§higes ML System
* Betrieb
* Abschluss / R√ºckblick

</textarea>
	</section>


	<section data-markdown>
		<textarea data-template>
## MLOps: Phasen und Workflow in einem ML Projekt

ML Projekte sind speziell
</textarea>
</section>

<section data-markdown class="fragments">
### Was ist MLOps?

* MLOps ist abgeleitet von DevOps
* Durch MLOps kommt ML in Produktion und wird in Betrieb gehalten
* Dazu kommen eine Reihe von Werkzeugen und Praktiken zum Einsatz
* √úberschneidung aus
  * Softwareentwicklung
  * Operations
  * Data Science
</section>

<section data-markdown class="fragments">
### Warum MLOps?

* im akademischen Leben z√§hlt f√ºr einen Wettbewerb h√§ufig nur der Score (G√ºte) des Modells
* dieser Ansatz hat sich im Bereich des Data Science auch in der Praxis breit gemacht
* die Praxis ist aber keine Kaggle Competition
* In-Sample Evaluation (auch wenn wir die vorher abgetrennt haben) sagt nur bedingt etwas f√ºr Eignung in
einer praktsichen Anwendung aus
* Out-Of-Sample Evaluation h√§ufig erst im produktiven Betrieb m√∂glich (evtl nur mitlaufen lassen)
</section>

<section data-markdown>
    <textarea data-template>
### ML Modelle brauchen permanente Wartung

<img src='img/mlops/modell-vergammelt.jpg'>

Das gilt nicht nur f√ºr ML Modelle, aber bei diesen ist es offensichtlicher
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Realistischer als mir lieb ist

By far, the most accurate representation of machine learning pipelines in the real world üçø üòÄ

https://twitter.com/bindureddy/status/1552073812157403136
</textarea>
</section>

<section data-markdown>
_MLOps today is in a very messy state with regards to tooling, practices, and standards. However, this is to be expected
given that we are still in the early phases of broader enterprise machine learning adoption. As this transformation
continues over the coming years, expect the dust to settle while ML-driven value becomes more widespread._

https://www.mihaileric.com/posts/mlops-is-a-mess/
</section>
		
	
<section data-markdown>
    <textarea data-template>
## Die Tool-Landschaft ist divers und meist (noch) Inhouse

<img src='img/mlops/mltools-ih.jpg' style="height: 400px;">

<small>

* https://towardsdatascience.com/lessons-on-ml-platforms-from-netflix-doordash-spotify-and-more-f455400115c7
* Diskussion: 
  * https://twitter.com/adamlaiacano/status/1458124198166122503
  * https://twitter.com/rahulj51/status/1455431014671699971

</small>

</textarea>
    </section>

<section data-markdown class="fragments">
### Ansatz des Workshops

* Der Bereich MLOps ist bisher weder im Bereich Framework noch konzeptionell standardisiert
* Ich zeige *eine* M√∂gliche L√∂sung f√ºr eine Problemstellung
* Meine eigene Erfahrung ist daf√ºr Grundlage
* Jeder gezeigte Ansatz kann von uns kritisch hinterfragt und bewertet werden
* Es gibt mit Sicherheit andere Ans√§tze, die auch gut funktionieren

</section>

<section data-markdown>
	<textarea data-template>

## Unser Fallbeispiel und Datenbasis

Daten entstehen √ºber die Zeit
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* _Ein lauff√§higes ML System_
* Betrieb
* Abschluss / R√ºckblick

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauff√§higes ML System
* _Betrieb / Monitoring_
* Abschluss / R√ºckblick

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Wir erinnern uns: ML Modelle brauchen permanentes Monitoring und Wartung

<img src='img/mlops/modell-vergammelt.jpg'>

</textarea>
</section>
<section data-markdown class="fragments">
### Grundregel

* Alle haben Probleme in Produktion
* Es gibt kein fehlerfreies System 
  * jedenfalls nicht f√ºr lange
* Ziel ist es, Fehler schnell 
  * zu entdecken
  * zu analysieren
  * und entsprechende ihrer Schwere zu adressieren

</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Woher wei√ü man, dass man ein neues Modell in Produktion braucht?

1. Schon in der Explorationsphase pr√ºfen wie sich das Modell auf neueren Daten verh√§lt
   * wie schnell degradiert die Performance?
   * Mindestens einmal im Jahr, damit man √ºberhaupt noch wei√ü wie es geht
1. Wenn die Metrik des Modells nachl√§sst in Produktion
   * Daf√ºr braucht man die Ground Truth der Daten aus Produktion
   * Manchmal bekommt man diese unmittelbar nach der Vorhersage durch die Reaktion eines menschlichen Benutzers
   * Oft aber auch erst nach nennenswerter Verz√∂gerung 
1. Wenn sich die Verteilung der Daten der Anfragen oder Vorhersagen deutlich von denen des Trainings unterscheiden 

</textarea>
	</section>

<section data-markdown>
	<textarea data-template>
### Monitoring mit Evidently, Prometheus und Grafana

<!-- <img src="img/mlops/monitoring-components.png"> -->
<img src="img/mlops/evidently_grafana_service.png">

<small>https://evidentlyai.com/blog/evidently-and-grafana-ml-monitoring-live-dashboards
<br>
https://docs.evidentlyai.com/integrations/evidently-and-grafana
</small>

</textarea>
</section>

<section data-markdown>
### Wir setzen einen eigenen Monitoring-Server auf

* Basiert auf Prometheus, Grafana und Evidently
* Code basiert auf https://github.com/evidentlyai/evidently/tree/main/examples/integrations/grafana_monitoring_service
* Nicht direkt Teil des eigentlichen Prod-Servers
* Zus√§tzliche Requests werden gegen den Monitoring-Server gemacht
* Requests √ºber die Zeit verteilt werden simuliert
</section>
	
<section data-markdown>
### Prometheus

Metric Server

* https://prometheus.io/
* https://prometheus.io/docs/prometheus/latest/getting_started/
  * https://prometheus.io/download/

</section>

<section data-markdown>
### Grafana

Visualisierung mit Dashboards
	
https://play.grafana.org/
</section>

<section data-markdown>
### Evidently

ML Performance Monitoring
	
https://evidentlyai.com/
</section>

<section data-markdown>
	<textarea data-template>
### √úbersicht der Services

* Monitoring App
  * Endpunkt f√ºr die Registrierung von Datens√§tzen: http://localhost:8085/iterate/<dataset\> (POST)
	* Nutzlast per JSON Body, kann Prediction enthalten z.B.: `[{"speed": 99.0, "age": 28.0, "miles": 21.0, "group": 2.0, "risk": 0.2110376180699707}]`  
  * Metrics aus Evidently: http://localhost:8085/metrics
    * √ºber `app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {"/metrics": prometheus_client.make_wsgi_app()})`

* Promethus: http://localhost:9090
* Grafana: http://localhost:3000

*Erfordert das Starten des Monitoring Services (kommt sp√§ter)*	
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Drift-Erkennung mit Prometheus, Evidently und Grafana

<img src="img/mlops/grafana-evidently-drift.png">

https://docs.evidentlyai.com/reports/data-drift
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wie erkennt man Drift?

* Es wird ein Statistischer Test auf den Eingabe-Daten ausgef√ºhrt
* Unsere Features sind numerisch und kategorisch
  * in `metrics_app/config.yaml` festgelegt
* Die Anfragen in Production werden verglichen mit Referenz-Datensatz, den wir zum Training benutzt haben (`datasets/insurance`)
* Evidently sucht als Default eine passende Metrik aus, es muss also nicht unser Problem sein
* Man kann aber auch von Hand konfigurieren, sowohl Test als auch Parameter
  * https://docs.evidentlyai.com/user-guide/customization/options-for-statistical-tests
  * https://docs.evidentlyai.com/user-guide/customization/options-for-data-target-drift

https://docs.evidentlyai.com/reports/data-drift

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Welcher Statistischer Test / welche Metrik?

es gibt leider nicht den einen passenden Test

* manche passen nur gut f√ºr kleine (< 1000) Datenmengen
  * unsere Datenmengen sind gr√∂√üer als 1000
* manche k√∂nnen nicht nur auf numerischen sondern kategorischen Daten arbeiten
  * wir brauchen beides
* manche sind zwischen 0 und 1 normiert
  * das ist uns eher egal
* unsere Metriken
  * Wasserstein Metrik f√ºr numerische Daten
  * Jensen-Shannon Distanz f√ºr kategorische Daten  

https://evidentlyai.com/blog/data-drift-detection-large-datasets
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Die Wasserstein-Metrik

_Wenn jede Verteilung als ein Haufen von ‚ÄûErde‚Äú angeh√§uft auf dem metrischen Raum betrachtet wird, dann beschreibt diese
Metrik die minimalen ‚ÄûKosten‚Äú der Umwandlung eines Haufens in den anderen._

* nicht zu sensitiv, zeigt nur gr√∂√üere Ver√§nderungen an
* normiert in Ver√§nderungen in Standardabweichungen
* kann (offensichtlich) √ºber 1 gehen
* ab 0.1 gehen wir von einem Drift aus
* funktioniert nur f√ºr numerische Daten

https://de.wikipedia.org/wiki/Wasserstein-Metrik
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Jensen-Shannon Divergenz

* Jensen-Shannon Distanz ist wie Wurzel aus der Divergenz, das ist unsere Metrik
* zwischen 0 und 1
* ab 0.1 gehen wir von einem Drift aus
* funktioniert auch f√ºr kategorische Daten
* basiert auf Kullback‚ÄìLeibler Divergenz, relative Entropie
* Histogramme werden verglichen, Gr√∂√üe des Samples daher egal
* Binning f√ºr kategorische Daten offensichtlich
* Intuition: wie viel Information/Entropie/√úberraschung steckt im Unterschied der beiden Verteilungen?


https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence
https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
</textarea>
</section>

<!-- <section data-markdown class="fragments">
	<textarea data-template>
### P-Werte

* die statistische Verteilung der jeweiligen Features
* weicht diese im Prod signifikant von der Verteilung im Training ab?
* diese Abweichung wird √ºber eine Metrik berechnet
* es kommt eine Konfidenz heraus, ob die Verteilungen unterschiedlich sind
* ab (default) 95% Konfidenz geht man von einer Abweichung aus 
* das bedeutet, dass es eine 5% Wahrscheinlichkeit gibt, dass die Verteilungen eigentlich doch gleich sind, man es aber nur gerade schlechte Beispiele sieht

https://de.wikipedia.org/wiki/P-Wert
</textarea>
</section> -->

<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauff√§higes ML System
* Betrieb
* _Abschluss / R√ºckblick_

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wir gehen zur√ºck in die Phasen I und II

* hier wird wieder in Notebooks gearbeitet
* die Bibliotheken werden inkludiert und bei jeder √Ñnderung neu geladen
* Jupyter Lab bietet eine gemeine Oberfl√§che f√ºr Notebooks und Bibliotheken
* Eine Kombination von Visual Studio Code und Jupyer Notebooks ist ebenso m√∂glich
<!-- * Selbst Colab erlaubt das Arbeiten auf einer Kombination von Notebooks und Bibliotheken
* Referenz: Module in Colab nutzen: https://colab.research.google.com/drive/1hDUO1-EzMVtVt6snmgrR1I1A36oPp4J9 -->
* R√ºckker in Phase I muss nicht radikal sein
* wenn Ansatz vergleichbar kann Phase II so erhalten bleiben
* neue Ergebnisse fliesen dann iterativ in die Professionalisierung

</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
### Zusammenfassung

1. Machine Learning Projekte k√∂nnen in Phasen gedacht werden
1. In der ersten Phase macht man m√∂glichst schnelle Experimente
1. Sollte sich eine Idee als tragf√§hig erweisen, professionalisiert man die Idee
1. Dies ist Voraussetzung und Grundlage f√ºr Produktion
1. In Produktion ergeben sich besondere Herausforderung im Bereich Monitoring
1. Typischerweise m√ºssen Machine Learning Systeme regelm√§√üig nachtrainiert und gepflegt werden
</textarea>
</section>

		<section data-markdown>
			<textarea data-template>
# Vielen Dank

MLOps mit Python und TensorFlow
Machine Learning betrachtet als ein Engineering Problem

M3 2022, https://www.m3-konferenz.de/veranstaltung-14132-0-mlops-mit-python-und-tensorflow.html

Bleibt gern im Kontakt

Oliver Zeigermann / oliver.zeigermann@openknowledge.de
https://www.linkedin.com/in/oliver-zeigermann-34989773/
https://twitter.com/DJCordhose

Lars R√∂wekamp / lars.roewekamp@openknowledge.de

Folien: https://bit.ly/m3-2022-mlops

</textarea>
		</section>

<section data-markdown>
# VON HIER AN NUR MATERIAL	
</section>




	<section data-markdown style="font-size: x-large;" class="hands-on">
		<textarea data-template>
## Hands-On 0 - Die Arbeitsumgebung √ºberpr√ºfen und kennen lernen

*Wir gehen zusammen durch das Notebook*

1. Wir empfehlen: arbeitet zusammen mit euren direkten Nachbarn
1. Sagt kurz Hallo
1. Falls das Projekt noch nicht installiert ist, dies bitte lokal tun, wie hier beschrieben unter Vorbereitung installieren: https://www.m3-konferenz.de/veranstaltung-14132-0-mlops-mit-python-und-tensorflow.html
1. `conda activate mlops-workshop`
1. `cd workspace`
1. `jupyter notebook`
1. http://localhost:8888/notebooks/insurance_ml.ipynb

https://www.m3-konferenz.de/veranstaltung-14132-0-mlops-mit-python-und-tensorflow.html

falls es Installationsprobleme gibt: https://colab.research.google.com/github/DJCordhose/insurance-ml/blob/main/workspace/insurance_ml.ipynb
</textarea>
	</section>


<section data-markdown>
	<textarea data-template>
## Unser Beispiel: Vorhersage von Risiken

* Wir sind CTO einer hochinnovativen Kfz-Versicherungsgesellschaft
* Anders als andere Versicherungsgesellschaften bestimmen wir den Tarif anhand der gesch√§tzen Anzahl von Unf√§llen pro Kunde
* Zielsetzung: Wie viele Unf√§lle werden die potenziellen Kunden haben?

<img src='img/pixabay/accident-151668_1280.png' style="height: 230px">
</textarea>
</section>

<section>
	<h3>Klassifizierung basierend auf bekannten Daten</h3>
	<img src="img/insurance-new/train-data.png" height="500px" class="fragment">
</section>


<section data-markdown>
	<textarea data-template>
### Vorhersage von Risiken f√ºr potenzielle Kunden

<a href='html/calculator.html'>
<img src='img/calculator.png' height="400">
</a>
<p><small>
<a href='html/calculator.html' target="_blank">
https://djcordhose.github.io/ml-resources/html/calculator.html</a></small>
</small></p>
</textarea>
</section>


	<section data-markdown>
		<textarea data-template>
# Agenda

* Phase 0: Grundlagen MLOps, Unser Beispiel, Installation
* _Phase 1: Exploration_
* Phase 2: Professionalisierung
* Phase 3: Produktion

</textarea>
	</section>

<section data-markdown class="fragments">
### Data SCIENCE

SCIENCE:

If you don't make mistakes, you're doing it wrong.

If you don't correct those mistakes, you're doing it really wrong.

If you can't accept that you're mistaken, you're not doing it at all.

https://twitter.com/ProfFeynman/status/1523317198168936448 	
</section>

<section data-markdown>
    <textarea data-template>
### Machine Learning k√∂nnen in Phasen strukturiert werden

<img src='img/sketch/phases-ml.png'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/twitter-fchollet-early-errors.png'>

https://twitter.com/fchollet/status/1525135062705983488
</textarea>
</section>


	<section data-markdown class="fragments">
### Phase 1: Exploration

* in der ersten Phase eines Machine Learning Projekts wird die Anwendungsidee validiert und ein
funktionsf√§higes Modell entwickelt.
* dabei ist ein schnelles iterieren und ausprobieren von Ideen zentral
* das Ziel ist *nicht* ein sinnvolles St√ºck Software
* Scripting passt hier besser als Programmieren als Ausdruck f√ºr die T√§tigkeit
* das Ziel ist eine schnelle Entwicklung
* Phase 1 endet entweder mit
  * einem funktionsf√§higen Modell mit dem man in Phase II √ºbergeht oder
  * dem Verwerfen des Ansatzes

	</section>

	<section data-markdown>
		<textarea data-template>
### Man muss sein Problem so formulieren, dass es f√ºr ML greifbar wird

<img src='img/software-complexity.png'>

<small>
Andrej Karpathy - TRAIN AI 2018 - Building the Software 2.0 Stack

https://vimeo.com/272696002

</small>
		</textarea>
	</section>

	<section data-markdown>
<textarea data-template>
### Literate Statistical Programming

1. Intent
1. Code
1. Data
1. Results
1. (Interpretation)

_Idee implementiert als sogenannte "notebooks"_

<small>https://en.wikipedia.org/wiki/Literate_programming</small>
<br>
<small>https://education.arcus.chop.edu/literate-statistical-programming/</small>

</textarea>
</section>

	<section data-markdown style="font-size: x-large;" class="hands-on">
		<textarea data-template>
## Hands-On I - Ein Modell erzeugen

*Arbeite weiter in deinem Notebook*

### Intro
1. Wir gehen noch einmal zusammen durch das Notebook und sehen uns die Teile genauer an
1. Adhoc-Intro ML und TensorFlow, so viel wie ihr braucht?
  1. Wer schon Bescheid wei√ü, kann direkt loslegen

### Modell Exploration  
1. Erzeuge ein gutes Modell
1. Wenn du willst und kannst, musst du nicht TensorFlow nutzen
1. In welchem Bereich ist das Modell zu gebrauchen?
1. Sorge daf√ºr, dass das Modell nur in diesem Bereich eine Vorhersage liefert
1. Wie k√∂nnte man sinnvoll mit einer Anfrage au√üerhalb des Wertebereichts umgehen?

Manche Leute m√∂gen https://mlflow.org/
</textarea>
	</section>

	<section data-markdown>
		<textarea data-template>
### Kann man sinnvoll direkt in Produktion gehen?

<img src='img/sketch/ml-phasen.png' style="height: 600px;">
</textarea>
	</section>


	<section data-markdown>
		<textarea data-template>
### Phase I hinterl√§sst gern einen Wust an Notebooks

<img src='img/sketch/notebook-explosion-no-title.png'  style="height: 600px;">

</textarea>
	</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Ist ein Wust an Notebooks ein Problem?

* das ist kein Zeichen von einem unprofessionellen Vorgehen
* ergibt sich aus der Arbeitsweise und Zielsetzung
* jeder Experimentator, erprobte ML Ansatz und jede Iteration kann eine neue, komplett entkoppelte Kopie eines Notebooks rechtfertigen
  * "Das Wichtigste in dieser Phase ist die schnelle Iteration" https://twitter.com/marktenenholtz/status/1488134981985583105
  * "Machen Sie ein einfaches Experiment nach dem anderen" https://karpathy.github.io/2019/04/25/recipe/
* nat√ºrlich wird dabei teilweise falsch entkoppelt
  * Wir kopieren alles, auch die Teile, die die beiden Notebooks gr√∂√ütenteils unver√§ndert teilen
  * Solange wir aber nicht wissen was die relevant gemeinsamen Teile sind m√ºssen wir damit weiter machen
* welcher Ansatz mehr Liebe verdient wird erst am Ende dieser Phase klar
</textarea>
	</section>

<!-- <section data-markdown>
<textarea data-template>
### Warum so viele unterschiedliche Experimente?

### Herausforderungen beim Training neuronaler Netze: 
## Das Training neuronaler Netze scheitert unbemerkt - die m√∂gliche Fehlerfl√§che ist gro√ü

### "Das Wichtigste in dieser Phase ist die schnelle Iteration." 
https://twitter.com/marktenenholtz/status/1488134981985583105
### "Machen Sie ein einfaches Experiment nach dem anderen" 
https://karpathy.github.io/2019/04/25/recipe/

</textarea>
</section>
 -->

<section data-markdown class="fragments">
<textarea data-template>
## Was will man da denn in Produktion bringen?

_ein Modell steht nicht f√ºr sich allein_
* es braucht Code f√ºr Vor- und Nachbearbeitung
* es ist eingebettet in andere Systeme

_wer will Scripte in Produktion?_
* Notebooks sind interaktive Scripte
* Wie ruft man das denn auf?
* Welches bringen wir in Produktion?
* Wie versionieren wir das? 
* Tests/Dokumentation?
* Debugging/Show References/Refactor/Autocomplete/Quick Fix/etc.?

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
# Agenda

* Phase 0: Grundlagen MLOps, Unser Beispiel, Installation
* Phase 1: Exploration
* _Phase 2: Professionalisierung_
* Phase 3: Produktion

</textarea>
</section>

	<section data-markdown>
		<textarea data-template>
<img src='img/mlops/mlops-systems.png'>					

<small>

https://twitter.com/nkoumchatzky/status/1525904101619417095
</small>

		</textarea>
	</section>



	<!-- <section data-markdown>
		<textarea data-template>
<img src='img/ml-bad.png'>					

<small>

https://twitter.com/DavidSKrueger/status/1487391569028296710
</small>

		</textarea>
	</section>
 -->
	<section data-markdown class="fragments">
### Phase 2: Professionalisierung

* in der zweiten Phase wird die skizzierte L√∂sung in ein langlebiges Projekt umgewandelt
* alle Regeln einer guten Software-Entwicklung gelten von nun an
* Stabilit√§t und Funktionalit√§t wird gew√§hrleistet
* Die Rahmenbedingungen der Produktionsumgebung m√ºssen erf√ºllt werden
* Art des Deployments, Sprache, Latenz, Speicher, Bandbreite, etc.
* Phase II endet entweder mit
  * reifem Code und Modell mit dem man in Phase III √ºbergeht oder
  * dem Iterieren zur√ºck in Phase I mit neu gewonnenen Erkenntnissen oder falls Rahmenbedingungen nicht erf√ºllt werden

	</section>


	<section data-markdown>
		<textarea data-template>
<img src='img/sketch/ml-dev-prozess.png' style="height: 650px;">
</textarea>
	</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Was geh√∂rt in Bibliotheken ausgelagert?

* Wir k√∂nnen nicht mit einem Notebook in Produktion gehen
  * also muss alles was wir in Produktion brauchen aus den Notebooks herausgezogen werden
* Bestimmte Teile eines Notebooks haben sich als stabil herausgestellt und sollten nicht bei jeder Kopie entkoppelt werden
* Alles was sich nach Software anf√ºhlt (Klassen, Funktionen, etc.) ist auch Software
  * diese Teile sollten auch wie solche behandelt werden  
* Professionalisierung muss gut abh√§ngen
  * Es stellt sich erst langsam heraus, was in ein Modul geh√∂rt
  * erste Version der exrahierten Module ist mit Sicherheit nicht endg√ºltig
</textarea>
	</section>

<section data-markdown>
### Scripte	
* Manche Notebooks werden in Phase II zu Skripten, die eine d√ºnne API um die Module sind. 
* K√∂nnen auch in CI/CD eingebaut werden.
* Training als Script ist oft nicht sinnvoll bei Deep Learning, da viel Anpassungen notwendig und besondere Ablaufumgebung mit nicht geteilten Ressourcen notwendig.

</section>

<section data-markdown>
### FAQ

* Wie sorgt man daf√ºr, dass die Notebooks nicht vergammeln und bei Anpassungen der lib nicht kaputt gehen
  * Antwort: gar nicht. Solange sie keine Scripte oder Module sind sind sie nicht stabil. 

</section>


	<section data-markdown style="font-size: x-large;" class="hands-on">
		<textarea data-template>
## Hands-On II - Entscheidende Teile in Bibliotheken auslagern

*Arbeite weiter in deinem Notebook*

### Intro (gemeinsam): Ein St√ºck Code aus dem Notebook herausziehen
1. https://ipython.org/ipython-doc/3/config/extensions/autoreload.html
1. Ein Python Modul erzeugen und im Notebook importieren
1. Tests und Debugging
   * mit https://docs.pytest.org/en/7.1.x/getting-started.html
   * von der Kommandozeile
   * in der IDE
1. Beispiele f√ºr Dokumentation in `InsuranceModel`
   * wo macht Doku Sinn?
   * Typen nutzen?  

### Kandidaten f√ºr das auslagern in Module ausmachen und auslagern
1. Was brauchen wir f√ºr Produktion?
1. Welche Teile verwenden wir immer wieder?
1. Was √§ndert sich nicht?

</textarea>
	</section>
<!-- 
	<section data-markdown>
		<textarea data-template>
### Referenz: Module in Colab nutzen

https://colab.research.google.com/drive/1hDUO1-EzMVtVt6snmgrR1I1A36oPp4J9
</textarea>
	</section>

<section data-markdown class="fragments">
#### Optional	
###	Gemeinsame Diskussion: Business Metrik ableiten

_Was k√∂nnte eine Business Metrik sein und kann man die quantitativ messen?_

* Wir haben technische Metriken zur Bewertung, aber was will man eigentlich erreichen?
* H√§tte man gern von Anfang an
* Geht aber mit der Entwicklung Hand-in-Hand
* H√§ufig hat man Anfang noch gar keine Ahnung, wie so eine Metrik aussehen w√ºrde
* Bzw. die Leute die das Dom√§nenwissen haben, kriegen das nicht formal sortiert
* Aufwand f√ºr die Sortierung lohnt sich evtl. gar nicht, weil man sowieso Phase I gar nicht verl√§sst oder zumindest nicht mit dem Ansatz
</section>
 -->

<section data-markdown>
	<textarea data-template>
# Agenda

* Phase 0: Grundlagen MLOps, Unser Beispiel, Installation
* Phase 1: Exploration
* Phase 2: Professionalisierung
* _Phase 3: Produktion_

</textarea>
</section>

	<section data-markdown>
		<textarea data-template>
<img src='img/everybody-gansta.png' style="height: 600px">

<small>

https://twitter.com/karpathy/status/1486215976559398915
</small>

</textarea>
	</section>

	<!-- <section data-markdown>
		<textarea data-template>
<img src='img/twitter-taylor-ml-deployment.png'>

<small>

https://twitter.com/SamuelDataT/status/1488150832742899718
</small>

</textarea>
	</section> -->

			<section data-markdown class="fragments">
### Phase 3: Produktion / Betrieb

in der dritten Phase wird die L√∂sung in Betrieb genommen

* alle Regeln des produktiven Einsatzes von Software gelten auch hier
* Monitoring hat zus√§tzliche Herausforderungen
  * Natur und Verteilung der Anfragen und auch Vorhersagen muss permanent √ºberwacht werden
* Phase III endet entweder mit
  * der Abschaltung 
    * entweder bald weil nutzlos oder
    * sp√§ter weil durch neues System ersetzt
  * dem Iterieren zur√ºck in Phase II mit neu gewonnenen Erkenntnissen
  * dem Iterieren zur√ºck in Phase I mit neu gewonnenen Erkenntnissen oder einem Neuansatz (h√§ufig ebenfalls ein Zeichen f√ºr einen Fehlschlag)

			</section>

	<section data-markdown class="fragments">
### Technische Umsetzung des Betriebs in Produktion

* Ein TensorFlow Graph l√§sst sich in unterschiedlichsten Szenarien einzusetzen
* Als Server
  * local
  * GCP
* Von C++
* Von JavaScript
* Von Java

	</section>


<section data-markdown class="fragments">
		<textarea data-template>
### Eine ML L√∂sung hat 2 Artefakte: Code und Modell		

* Beides muss in Prod gebracht werden
* Daten und Modell muss extern gehalten, aber zusammen mit Code versioniert werden
* Modell beschreibt einen Ausschnitt der Realit√§t. Wie finde ich heraus wie gut es das tut uns vor allem f√ºr relevante Teile der Welt und was mache ich wenn sich das √§ndert
* Richtung: Benchmark und Monitoring der L√∂sung in Produktion
<!-- * Prod geht auf Code und Modell Binaries, Modell Graph √ºberall ausf√ºhren -->

</textarea>
	</section>

	<section data-markdown>
		<textarea data-template>
### Nicht aller Code ist f√ºr Produktion gedacht		

* Was geht in Produktion
  * Vorhersage
  * Monitoring

* Was geht nicht in Produktion
  * Training
  * Analytics
  * Visualisierung


</textarea>
	</section>

	<section data-markdown>
		<textarea data-template>
### Gro√üe Dateien versionieren

Sinnvoll f√ºr Trainingsdaten und Modell 

* git lfs: https://git-lfs.github.com/
* DVC: https://dvc.org/
		</textarea>
	</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIa - Intro der Produktionsumgebung

1. Im Ordner `app` ist eine Server-Anwendung vorbereitet
1. In `app.py` ist ein einfacher Flask Server implementiert
1. `cd app`
1. Start √ºber `python app.py`
1. app.py benutzt eine zus√§tzliche lib, in der du den Pfad auf dein eigenes Modell anpassen musst
1. F√ºr HTML clients Web Server auf 8000 starten: `python -m http.server 8000`
1. http://localhost:8000/app/client.html macht einen einfachen Requests
1. http://localhost:8000/docs/html/calculator.html ist ein interactiver Client (Remote Call anhaken)
</textarea>
	</section>

	<!-- <section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIa - Intro Docker Umgebung

1. `Dockerfile` enth√§lt die Bauanleitung f√ºr das Docker Image
1. `docker-compose up --build` (Referenz https://docs.docker.com/compose/reference/up/)
   * baut und startet den Server
   * started Prometheus und Grafana
   * Definition in `docker-compose.yaml`
1. `docker-compose down` stopt alle Services wieder   

https://github.com/DJCordhose/insurance-ml/blob/main/README.md   
</textarea>
	</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIa - Das Modell in Produktion bringen

1. `app.py` so anpassen, dass damit das eigenen Programm instrumentiert wird 
1. Starten und mit `client.html` ausprobieren
1. Optional: Dassselbe mit der Docker-Umgebung 

</textarea>
	</section>
 -->
	<section data-markdown>
	<textarea data-template>
# Agenda

* Phase 0: Grundlagen MLOps, Unser Beispiel, Installation
* Phase 1: Exploration
* Phase 2: Professionalisierung
* _Phase 3b: Produktion / Monitoring_

</textarea>
</section>

<!-- <section data-markdown class="todo">
### Monitoring ist eine Art Predictive Maintainace
</section> -->

<!-- <section data-markdown>
### Die vier goldenen Regeln des traditionellen Monitorings

* Latency
* Traffic
* Errors
* Saturation

https://sre.google/sre-book/monitoring-distributed-systems/	
</section>
 -->

 <section data-markdown>
    <textarea data-template>
### Wir erinnern uns: ML Modelle brauchen permanente Wartung

<img src='img/mlops/modell-vergammelt.jpg'>

</textarea>
</section>


<section data-markdown style="font-size: x-large;">
	<textarea data-template>
### Monitoring f√ºr ML muss komplexer sein

<img src="img/google-ml-monitoring.png">

Long: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf
Short: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45742.pdf
</textarea>
</section>

<!-- <section data-markdown class="fragments";>
## Welche Metriken kann man in ML Systemen √ºberwachen?

Entscheidend ist die Performance des Systems

* Technische Metriken
  * ohnehin verf√ºgbar, weil f√ºr das Training erforderlich
  * accuracy, precision/recall 
* Business Metriken
  * z.B.
	* Umsatz / Gewinn
	* Umfang Schadensmeldungen
    * allgemeiner K√§ufe, Views, Clicks, etc.
  * oft sinnvoller, aber schwerer zu messen oder schwerer zu quantifizieren

</section> -->

<!-- <section data-markdown class="fragments">
## Das reicht meist nicht

* Ground Truth liegt erst sehr sp√§t vor
  * Ob jemand ein guter Kunde ist wissen wir oft erst nach Jahren
* Manchmal liegt die Ground Truth √ºberhaupt nicht vor
* Filter Bubble: es werden nur bestimmte Daten gesammelt
  * wir sammeln nur Daten von Leuten, die auch unsere Kunden werden   
* Unterschiedliche Teilbereiche k√∂nnen eine unterschiedliche Performanz haben
  * W√§hrend ein Bereich besser wird, kann ein anderer, evtl. kleinerer Bereich deutlich schlechter werden, bei gleichbleibenden Performance
* Performance kann stark √ºber die Zeit variieren, ohne dass es ein Problem gibt
  * anh√§ngig von der Art der Anfragen
  * manchmal mit Regelm√§√üigkeiten innerhalb eines Tages
  * Trend in Abweichungen manchmal schwer zu bestimmen   
</section>


<section data-markdown class="fragments">
## Was kann man machen: Fr√ºhes Monitoring

* Qualit√§t der Daten
  * wie ver√§ndern sich fehlende oder falsche Felder
  * Plausibilit√§t
* Daten Drift
  * Verteilung der Eingabedaten
* Prediction Drift   
  * Was gibt das Modell aus?
</section>

<section data-markdown>
## Datenqualit√§t

* Felder
  * fehlen
  * ung√ºltig
  * falsch / unplausibel / Wertebereich verlassen
* Features 
  * konstante (die (meisten) Eingaben haben (fast) denselben Wert)
  * leere
  * fast leer
  * Korrelationen zwischen Features
</section>
	
 -->
<section data-markdown class="fragments">
	<textarea data-template>
### Wir lassen das System ein bisschen in Produktion laufen

Mal sehen wie sich das System macht

- Information √ºber Schadensf√§lle neuer Kunden kommen nur verz√∂gert
- Aber neue Meldungen √ºber Schandensmeldungen kommen permanent
- Wir haben keine explizite Kontrolle dar√ºber, wer bei uns versichert werden will und wessen Unfalldaten wir bekommen
- Es gibt aber eine Tendenz dahin, dasss eher Kunden mit guten Konditionen kommen

<!-- <small>

https://colab.research.google.com/github/DJCordhose/ml-resources/blob/main/notebooks/mlops/3_mlops_shift.ipynb
</small> -->

</textarea>
</section>


<section data-markdown>
	<textarea data-template>

<!-- <img src='img/6monts-later.jpg' height="600px"> -->
<img src='img/2year-later.jpg' height="600px">

</textarea>
</section>


<section data-markdown data-transition="none">
	<textarea data-template>
## Ergebnis des Modells nach zwei Jahren

<img src='img/insurance-new/insurance-after-shift.png' class="fragment">

</textarea>
</section>


<section data-markdown data-transition="none">
	<textarea data-template>
## Urspr√ºngliche Daten zum direkten Vergleich

<img src='img/insurance-new/insurance-pred.png'>

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Was ist hier passiert? 

*Die Welt steht nicht still - Model und Welt laufen auseinander, aus 70%  Genauigkeit werden 65%*

* Elektroautos finden weitere Verbreitung
* potente Elektroautos haben allgemein deutlich geringere H√∂chstgeschwindigkeit 
* aber super Beschleunigung
* Gute Beschleunigung ist viel eher Ursache f√ºr rasante Fahrweise, Unfallwahrscheinlichkeit ist hoch
* Wir haben aber nur H√∂chstgeschwindigkeit als Daten (seht im Fahrzeugschein), Korrelation war angenommen
* Der Cluster mit jungen, schlechten Fahrern ist nach unten gerutscht
* _Wird nun f√§lschlich als gut vorhergesagt und werden g√ºnstig versichert_
<!-- * Tats√§chlich sehen wir aber viele Unf√§lle -->
<!-- - Faherer vielleicht ein bisschen √§lter geworden -->
</textarea>
</section>

<!-- <section data-markdown>
	<textarea data-template>
<img src='img/model-eval.png'>

<small>

https://twitter.com/marktenenholtz/status/1528021809697792003</small>
		</textarea>
</section>
	 -->

<!-- <section data-markdown class="fragments">
	<textarea data-template>
### Wie wird √ºberpr√ºft?

* die statistische Verteilung der jeweiligen Features
* weicht diese im Prod signifikant von der Verteilung im Training ab?
* diese Abweichung wird √ºber eine Metrik berechnet
* es kommt eine Konfidenz heraus, ob die Verteilungen unterschiedlich sind
* ab (default) 95% Konfidenz geht man von einer Abweichung aus 
* das bedeutet, dass es eine 5% Wahrscheinlichkeit gibt, dass die Verteilungen eigentlich doch gleich sind, man es aber nur gerade schlechte Beispiele sieht

https://de.wikipedia.org/wiki/P-Wert
</textarea>
</section>
 -->
<!-- <section data-markdown>
	<textarea data-template>
## Die Metrik

### in unserem Fall wird die Wasserstein-Metrik f√ºr den Vergleich gew√§hlt

_Wenn jede Verteilung als ein Haufen von ‚ÄûErde‚Äú angeh√§uft auf dem metrischen Raum betrachtet wird, dann beschreibt diese
Metrik die minimalen ‚ÄûKosten‚Äú der Umwandlung eines Haufens in den anderen._

https://de.wikipedia.org/wiki/Wasserstein-Metrik
</textarea>
</section> -->

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIb - Monitoring und Sinn daraus machen

### Teil I: Das Monitoring starten 

1. Starte die Monitoring Umgebung in `monitoring` √ºber 
   * `docker compose up -d --build`
1. Checke, dass diese Services laufen
   * Monitoring App, Metrics aus Evidently: http://localhost:8085/metrics
   * Promethus: http://localhost:9090
   * Grafana: http://localhost:3000

</textarea>
	</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIb - Monitoring und Sinn daraus machen

### Teil II: Request simulieren

1. Einloggen in Grafana: http://localhost:3000
   1. admin/admin
1. Das Dashboard `Evidently Data Drift Dashboard` aufrufen
1. Darin den Datensatz `insurance`
1. Mit `python scripts/example_run_request.py` Anfragen simulieren
1. Stelle sicher, dass sich das Dashboard aktualisiert

</textarea>
	</section>

<section data-markdown style="font-size: x-large;">
	<textarea data-template>
## Drift erfordert Interpretation

Wenn die Welt sich √§ndert, ist Drift zu erwarten und damit ok

|   | Positive Interpretation, keine Ma√ünahme erforderlich  | Negative Interpretation, Ma√ünahme erforderlich  |
|---|---|---|
| *Data und Prediction Drift*  | wichtige Features haben sich ge√§ndert, Modell kommt klar und extrapoliert gut, z.B.: H√∂heres Alter, mehr Risiko  |  wichtige Features haben sich ge√§ndert, Modell extrapoliert nicht sinnvoll |
| *Data aber kein Prediction Drift*  | keine wichtigen Features ge√§ndert, das Modell ist robust genug f√ºr den Drift  | wichtige Features ge√§ndert, Modell extrapoliert nicht sinnvoll |
| *Prediction aber kein Data Drift*  | ???  | wichtige Features ge√§ndert, die Metric hat das aber nicht detektiert |
|   |   |   |

</textarea>
</section>

<section data-markdown>
## Alarm bei Data Drift

* Ver√§ndert sich die Art der Eingabedaten?
  * Oft ausgedr√ºckt durch die Verteilung der einzelnen Features

* Muss nicht notwendig schlimm sein
  * Wenn entscheidende Features betroffen sind, kann es ein Problem sein 
* Alarm anhand von
  * Wichtigkeit der driftenden Features
  * Anzahl der driftenden Features
  * Ausma√ü des Drifts
<!-- * Daf√ºr wichtig
  * Vern√ºnftige Tests anhand der Metriken
  * Vern√ºnftige Konfidenzintervalle -->

</section>


<section data-markdown>
## Ma√ünahmen bei Drift

* Neue Daten aufnehmen (und labeln)
* Neue Version des Modells trainieren
* Schnelle Ma√ünahme
  * Pre-/Post-Processing des Modells neu kalibrieren
  * Schwellwerte f√ºr Anwendung anpassen
  * Bestimmte Bereiche ausklammern 
  * Modell Architektur √§ndern (oder fixen) und neu trainieren
* Sehr schnelle Ma√ünahme: Fallback
  * Manuell
  * Heuristik / Baseline
</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIb, Monitoring und Sinn daraus machen

### Teil III: Einen passenden Grafana Alert erzeugen

1. Einen Alert kann man in Grafana als Regel oder direkt auf einem Dashboard einrichten
1. Editiere im Dashboard entweder das Panel f√ºr die P-Werte oder f√ºr den Anteil driftender Features
1. Im Alert Tab kannst die Bedingung f√ºr den Alarm festlegen
1. Eventuell musst du erst einen Ordner erzeugen: https://grafana.com/docs/grafana/latest/dashboards/dashboard_folders/
1. Nach dem Speichern sollte ein farbiges Herz auf dem Panel den Status angeben
1. Stelle sicher, dass dein Alert ausgel√∂st wird
1. Im Alerting Menu k√∂nnen Contact Points zur Benachrichtigung angelegt werden

https://grafana.com/docs/grafana/latest/alerting/

</textarea>
	</section>

<!-- <section data-markdown>
## Mehr √ºber Monitoring
</section> -->

<!-- <section data-markdown>
### Man hat immer Verz√∂gerung bis man wei√ü, ob die Vorhersagen gut waren

* Wenn man warten kann bis die da sind hat man einen Vorteil
* manchmal erst nach Jahren
* Und manchmal wei√ü man es aber nie
</section> -->

<!-- <section data-markdown>
## Drift Detection
</section>

<section data-markdown>
## Metriken f√ºr Drift 

* Wie viele Predictions sind g√ºltig bzw. werden als gut genug angesehen?
* Art der Verteilung (z.B. Normal)
* Parameter einer angenommenen Normalverteilung
  * Mean
  * Std Dev
  * Percentile
  * min-max
* Statistische Tests f√ºr Konfidenzintervalle
  * KL Divergence
  * Kolmogorow-Smirnow-Test  	
</section>
 -->

<!-- <section data-markdown>
## Vern√ºnftige Tests: Nicht-Parametrische Tests

</section>


<section data-markdown>
## Vern√ºnftige Tests: Parametrische Tests

* Parametrische Tests sind besser als Nicht-Parametrische f√ºr Drift-Detection
* M√ºssen f√ºr jedes Feature einzeln aufgesetzt werden
* Sinnvoll wenn man nicht zu viele Features hat und man eine hohe Sicherheit braucht
* Beispiele f√ºr Tests
  * Z-Test
    * Es wird eine Normalverteilung angenommen
    * https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/z-test/
    * Z-Test, T-Test des Mittelwerts (m = m0)
    * Z-Test f√ºr eine Proportion https://statologie.de/z-test-eine-proportion/ (p = p0)
</section>
 -->

	<!-- <section data-markdown class="fragments">
		<textarea data-template>
### Probleme: Selection/Survivor Bias

* Wir k√∂nnen nicht aktiv Daten sammeln, sondern nur von unseren Kunden
* Wir haben keine explizite Kontrolle dar√ºber, wer bei uns versichert werden will und wessen Unfalldaten wir bekommen
* Es gibt aber eine Tendenz dahin, dasss eher Kunden mit guten Konditionen kommen
* M√∂gliche L√∂sung: ab und zu risikoreichen Personen gute Angebote machen
* Neues Problem: potentielle Nachvollziehbarkeit

https://en.wikipedia.org/wiki/Survivorship_bias
</textarea>
	</section>  -->

	<!-- <section data-markdown class="fragments todo" style="font-size: x-large;">
### Phase 3: Betrieb

* Schwierigkeit:
* M√ºssen wir wieder zur√ºck ins Experiment?
* Wie finden wir heraus, ob unser Modell gut funktioniert?

* Monitoring
* Alles was man auch sonst monitort
* Plus Anfragen sammeln und Vorhersagen mitschreiben

* Achtung Bias
* Wenn wir nur die vermeintlich guten Kunden annehmen, wie k√∂nnen wir dann einen schlechten erkenen?
* wir haben ja gar keinen oder nur wenige als Kunden
* Einen gewissen Prozentsatz mit schlechter Hypothese ein sehr gutes Angebot machen

	</section> -->

	<!-- <section data-markdown class="todo">

- Information √ºber Schadensf√§lle neuer Kunden kommen nur verz√∂gert
- Aber neue Meldungen √ºber Schandensmeldungen kommen permanent
- wie oft neu trainieren?
- mit welchen Daten? Aktualit√§t vs Datenmenge?
- wie schnell √§ndert sich die Welt? wie schnell die Menschen die bei uns Kunden sein wollen?
- Datens√§tze schnell statistisch vergleichen mit describe
	</section>
-->


	<!-- <section data-markdown>
		<textarea data-template>
### Prozess eines ML-Projekts

<img src='img/sketch/phases.png' style="height: 100%;">    
</textarea>
	</section>


	<section data-markdown>
		<textarea data-template>
### Wieso MLOps oder warum ist ein Modell nie wirklich fertig

_Concept und Data Drift_
- Vorhersagen werden ohne Nachtraining schlechter 
- die Welt entwickelt sich weiter und liefert andere Daten

_Passiert_
- pl√∂tzlich (neue oder ver√§nderte Konkurrenten, schwerwiegende Ereignisse) oder
- schleichend (gesellschaftliche Entwicklungen)

unterschiedliche Abschnitte der Daten k√∂nnen unterschiedlich schnell vergammeln

<small>

https://en.wikipedia.org/wiki/Concept_drift
<br>
https://twitter.com/chipro/status/1313921889061015557

</small>
</textarea>
	</section> -->			

	<!-- <section data-markdown>
### Ein vielleicht noch weiter verbreitetes Fehlerbild

Nicht die Welt √§ndert sich, sondern unsere Kodierung davon

* Beispiel Bild-Daten
  * float 0..1 vs int 0..255
  * wie viele Farbkan√§le
* insbesondere Neuronale Netze haben "silent failure"
* also: Wertebereich checken und Typ der Argumente
* bei unseren Beispiel: normalisiert vs nicht normalisiert
* Was monitoren
  * Verteilung
  * Kodierung

</section>
 -->




		</div>
	</div>
	<script src="revealjs/reveal.js/dist/reveal.js"></script>
	<script src="revealjs/reveal.js/plugin/notes/notes.js"></script>
	<script src="revealjs/reveal.js/plugin/markdown/markdown.js"></script>
	<script src="revealjs/reveal.js/plugin/highlight/highlight.js"></script>
	<script src="revealjs/config.js"></script>


</body>

</html>