<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=10.0, user-scalable=yes">

	<title>Workshop: MLOps MAD 2022</title>

	<link rel="stylesheet" href="revealjs/reveal.js/dist/reset.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/reveal.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/theme/white.css" />

    <!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/monokai.css"> -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/zenburn.css"> -->
    <link rel="stylesheet" href="revealjs/highlight-js-github-theme.css" />
    <link rel="stylesheet" href="revealjs/styles.css" />

</head>

<body style="background-color: whitesmoke;">
	<div class="reveal">
		<div class="slides">

<!-- 
https://mad-summit.de/fundamentals/mlops-wo-machine-learning-auf-softwareentwicklung-trifft/

MLOps ‚Äì wo Machine Learning auf Softwareentwicklung trifft

Machine Learning kann sowohl mit dem Ziel des Erkenntnisgewinns als auch mit dem Ziel einer Vorhersage betrieben werden.
W√§hrend Erkenntnisgewinne eher im Bereich Data Science oder Statistik angesiedelt sind, kann eine Vorhersage auch wie
ein klassisches St√ºck Business-Logik eingesetzt werden. In diesem Bereich der Vorhersagen gibt es viele √úberschneidungen
mit klassischer Softwareentwicklung und der DevOps-Bewegung, allerdings auch einige Besonderheiten.

In diesem Workshop sehen wir uns unabh√§ngig von der Programmiersprache den Bereich des Machine Learnings durch die
Brille der Softwareentwicklung an.
-->

<section data-markdown class="todo">
	<textarea data-template>
### Erkenntnisse nach Fortiss Hotspot MLOps

1. Story klarer kriegen
  * Drift detektieren bevor man ground truth hat
  * ODER
  * Detektieren wann man neue Ground Truth sammeln und neu trainieren sollte

2. Deployment Strategien kl√§ren
  1. Deploy Permanently VS
  2. Deploy when you need to
  
3. In Iterationen vorgehen
  1. Erst einmal die Story komplett durchgehen
  1. Dann Details
  </textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Warum Drift detektieren?
-     
  </textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### √úbung

* Welches Modell bringen wir in Prod
* 80% ist Minimum
* Weitere Modelle basteln lassen die daf√ºr die Grundlage bilden
</textarea>
</section>



<section data-markdown class="todo">
	<textarea data-template>
### Bias / Fairness

* √úber der Oberfl√§che
  * Widerspr√ºchliche Metriken
  * Accuracy-Fairness-Tradeoff
  * Bias bereits in Trainingsdaten
  * Wer entscheidet was Fair ist?
  * Gruppenbasierte vs individuelle Fairness
* Unter der Oberfl√§che
  * Proxy Variablen (nicht Einkommen oder Alter, sondern PLZ)
  * Data- oder Concept-Drift
  * Ist eine algorithmische L√∂sung m√∂glich und angemessen
  * Einspruch gegen algorithmische Entscheidung m√∂glich?
  * Keine Ground-Truth in Production
  * Datenschutz vs Fairness
  * N√ºtzt Fairness dem Gesch√§ft?
  * Beim manuellem Labelling k√∂nnen Labels Weltanschauung in Bias der Annotierenden enthalten
  * Zementierung bestehender Machtstrukturen
  * Unm√∂glichkeit eindeutiger Kategorisierung (z.B. Geschlecht, sexuelle Orientierung)
</textarea>
</section>

<section data-markdown class="todo">
	<textarea data-template>
### Range Check

* Limit of Range Check
  * Triangle
  * Blob
* Outlier Detection  
</textarea>
</section>


<section data-markdown class="preparation">
	<textarea data-template>
### Vorbereitung

* Stift einsatzbereit (geladen)
* Probelauf Drift Detection
* Installation einmal platt machen: `docker compose down --volumes --rmi all`
* Und wieder hoch fahren dabei alles neu bauen und alle images neu ziehen: `docker compose up --build`
* Kopie von Workshop Miro machen und RW und mit allen teilen auf erster Folie
* WLAN eintragen
</textarea>
</section>
`
<section data-markdown style="font-size: x-large;">
	<textarea data-template>
### Bevor es los geht

1. WLAN: XXX
1. Diese Folien: XXX (erste Folie ist aus Versehen leer, einmal vor und zur√ºck)
1. Miro f√ºr gemeinsame Arbeit: https://miro.com/app/board/uXjVPWwLbm8=/?share_link_id=484469168010

_Bei Problemen den Nachbar oder Tobias und Olli fragen_
</textarea>
</section>

			<section data-markdown>
				<textarea data-template>
# MLOps ‚Äì wo Machine Learning auf Softwareentwicklung trifft

MAD 2022, https://mad-summit.de/fundamentals/mlops-wo-machine-learning-auf-softwareentwicklung-trifft/

Hanna L√ºschow / hanna.lueschow@openknowledge.de

Oliver Zeigermann / oliver.zeigermann@openknowledge.de

Folien: https://bit.ly/mad-2022-mlops
    </textarea>
			</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Hanna

<img src='img/Hanna-Lueschow-6.jpg'>

<p>
<a target="_blank" href="mailto:hanna.lueschow@openknowledge.de">Hanna L√ºschow</a>:
Dev@OPEN KNOWLEDGE
</p>    
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Olli

<img src='img/olli-opa.jpeg'>

<p>
<a target="_blank" href="mailto:oliver.zeigermann@openknowledge.de">Oliver Zeigermann</a>:
ML@OPEN KNOWLEDGE
</p>    
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Wer seid ihr?

* Was macht ihr?
* Was wisst ihr schon?
* Warum seid ihr hier?
</textarea>
</section>

	<section data-markdown>
		<textarea data-template>
# Agenda

1. Unsere Story aus der Vogelperspektive

</textarea>
	</section>

	<section data-markdown>
		<textarea data-template>
# Story

1. Wir sind CTO einer Kfz-Versicherung und wollen anhand von Risiko versichern
1. Gute Fahrer sollen bessere Konditionen bekommen
1. Das erfordert die Vorhersage des Risikos anhand von ausgew√§hlten Kriterien
1. Wir gehen davon aus, dass wir das tats√§chliche Risiko eines Versicherten erst nach einem Jahr realistisch bewerten k√∂nnen

</textarea>
	</section>


  <section data-markdown style="font-size: xx-large;" class="hands-on">
    <textarea data-template>
## Hands-On 1 - Exploration gemeinsam durchgehen

* Neuronales Netzwerk mit TensorFlow
* 3 Hidden Layers, 100 Neuronen pro Layer
* 1500 Datens√§tze insgesamt
* Training auf allen 6 Features
* Test/Validation auf 300 Datens√§tzen
* Training auf 1200 Datens√§tze
* Normalisierung auf Trainingsdatens√§tzen
* Accuracy Training/Test > 85%

Aus Docker: http://localhost:8888/notebooks/notebooks/exploration.ipynb
<br>
Colab Fallback (falls die Installation noch l√§uft): https://colab.research.google.com/github/openknowledge/mlops-data2day/blob/main/notebooks/exploration.ipynb

  </textarea>
  </section>  


	<section data-markdown>
		<textarea data-template>
# Agenda

* _Grundlagen MLOps, Unser Beispiel_
* Ein lauff√§higes ML System
* Betrieb
* Abschluss / R√ºckblick

</textarea>
	</section>

  <section data-markdown>
    <textarea data-template>
  ### MLOps
  
  * KI ist keine Magie
  * KI ist eher "k√ºnftig Informatik"
  * Machine Learning is die zur Zeit wichtigste Technik im Bereich KI
  * Machine Learning ist eine alternative Art, Software zu entwickeln
  * Software soll in Produktion gebracht und dort gehalten werden 
  * Daher sollten wir die bekannten Techniken der Software Entwicklung auf Machine Learning anwenden
  
  </textarea>
  </section>
  
 <section data-markdown>
  <textarea data-template>
### Machine Learning Anwendungen brauchen Wartung

<img src='img/verfall-1.PNG'>

Das gilt nicht nur f√ºr ML Anwendungen, aber bei diesen ist es offensichtlicher
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Machine Learning Anwendungen brauchen Wartung

<img src='img/verfall-2.PNG'>

Das gilt nicht nur f√ºr ML Anwendungen, aber bei diesen ist es offensichtlicher
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Realistischer als mir lieb ist

By far, the most accurate representation of machine learning pipelines in the real world üçø üòÄ

https://twitter.com/bindureddy/status/1552073812157403136
</textarea>
</section>

<section data-markdown>
	<textarea data-template>

Honestly: sometimes I feel defeated because ML observability is so hard. All facets are hard -- detecting, diagnosing, reacting to bugs. We don't have realtime ground truth labels (except recsys) so we don't know asap when performance goes down. Lots of $$ left on the table (1/6)

https://twitter.com/sh_reya/status/1539420163480489984? 
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Typische Werkzeuge f√ºr Workflows und Orchestrierung

* Java, allgemein: https://www.jenkins.io/
* TensorFlow, Tfx: https://www.tensorflow.org/tfx
  * https://www.tensorflow.org/tfx/guide/beam_orchestrator
  * https://www.tensorflow.org/tfx/guide/airflow
  * https://www.tensorflow.org/tfx/guide/kubeflow
* Schie√üen wie Pilze aus dem Boden

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Gro√üe Dateien versionieren

Sinnvoll f√ºr Trainingsdaten und Modell 

* git lfs: https://git-lfs.github.com/
* DVC: https://dvc.org/
	</textarea>
</section>

<section data-markdown>
### MLOps Is a Mess But That's to be Expected

* MLOps today is in a very messy state with regards to tooling, practices, and standards. 
* However, this is to be expected given that we are still in the early phases of broader enterprise machine learning adoption. 
* As this transformation continues over the coming years, expect the dust to settle while ML-driven value becomes more widespread.

https://www.mihaileric.com/posts/mlops-is-a-mess/
</section>
		
	
<section data-markdown>
    <textarea data-template>
## Die Tool-Landschaft ist divers und meist (noch) Inhouse

<img src='img/mlops/mltools-ih.jpg' style="height: 400px;">

<small>

* https://towardsdatascience.com/lessons-on-ml-platforms-from-netflix-doordash-spotify-and-more-f455400115c7
* Diskussion: 
  * https://twitter.com/adamlaiacano/status/1458124198166122503
  * https://twitter.com/rahulj51/status/1455431014671699971

</small>

</textarea>
    </section>

<section data-markdown class="fragments">
### Ansatz des Workshops

* Der Bereich MLOps ist bisher weder im Bereich Framework noch konzeptionell standardisiert
* Wir spezialisieren uns nicht auf eine Art der Workflow Ausf√ºhrung
* F√ºr diesen Workshop sind wir selbst der Orchestrator und f√ºhren die einzelnen Schritte h√§ndisch aus

</section>

<section data-markdown>
  <textarea data-template>
### Was ist Docker?

* Einheitliche, leichtgewichtige Form der Virtualisierung
* Es werden Images gebaut, die die ben√∂tigte Software beinhalten und diese werden in einem Container ausgef√ºhrt
* Container isolieren die Software von der Umgebung und sind unabh√§ngig von der Infrastruktur
* "docker compose" l√§sst uns mehrere Container gleichzeitig starten

<img src='img/docker-vm-vergleich.png'  style="height: 250px;">

https://www.docker.com/resources/what-container/

  </textarea>

</section>

<section data-markdown style="font-size: large;" class="hands-on">
	<textarea data-template>
## Hands-On 0 - Docker Installation √ºberpr√ºfen oder herstellen

Helft bitte euren Nachbarn, falls eure Installation schon l√§uft

1. Wir empfehlen: arbeitet zusammen mit euren direkten Nachbarn
1. Stellt sicher, dass zumindest einer eurer Rechner eine lauff√§hige Installation hat 
1. Sagt kurz Hallo
1. Installieren (falls noch nicht passiert)
   1. Git : https://git-scm.com/downloads
   1. Docker: https://docs.docker.com/get-docker/
1. Projekt klonen
   * https://github.com/openknowledge/mlops-data2day
   * `git clone git@github.com:openknowledge/mlops-data2day.git`
1. Docker images starten
   * Im Projekt-Verzeichnis: `docker compose up`
   * Wir brauchen die Ports: 8000, 8080, 9090, 8085, 8888, 8889, 9090, 3000
   * Das kann (gerade bei √ºberlastetem Netz) lange dauern
   * sobald das Docker-Kommando abgesetzt ist k√∂nnen wir weiter im Stoff
   * Zwingend brauchen wir die Services erst in Hands-On IIb 
1. Nachdem Docker alle Container gestartet hat √ºberpr√ºfen, dass Jupyter erreichbar ist: http://localhost:8888/   

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### √úbersicht Container

<img src="img/containers-desktop.png">

Das sollte bei dir laufen
</textarea>
</section>


<section data-markdown>
	<textarea data-template>

## Unser Fallbeispiel und Datenbasis

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Unser Beispiel: Vorhersage von Risiken

* Wir sind CTO einer hochinnovativen Kfz-Versicherungsgesellschaft
* Anders als andere Versicherungsgesellschaften bestimmen wir den Tarif anhand der gesch√§tzen Anzahl von Unf√§llen pro Kunde
* Zielsetzung: Wie viele Unf√§lle werden die potenziellen Kunden haben?

<img src='img/pixabay/accident-151668_1280.png' style="height: 230px">
</textarea>
</section>

<section data-markdown>
### Features
* Fahrer
  * *training*: 0/1, hat der Fahrer ein Fahrertraining absolviert
  *	*age*: Alter des Fahrers in Jahren
* Fahrzeug  
  *	*emergency_braking* (sic!): 0/1, hat das Fahrzeug ein Notbremssystem 
  *	*braking_distance*: Bremsweg aus Tempo 100
  * *power*: Leistung in KW
*	*miles*: J√§hrliche Fahrleistung in Meilen
* Das wollen wir vorhersagen
  *	*risk*: Unfallrisiko, nach unten und nach oben offen, kann negativ sein
  *	*group* / *group_name*:	Einteilung in Gruppe anhand des Risikos
</section>

<section data-markdown>
	<textarea data-template>
## Daten entstehen √ºber die Zeit

Die Ground Truth (GT) kommt verz√∂gert und kann sich √ºber die Zeit noch ver√§ndern

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Daten entstehen √ºber die Zeit

Die Ground Truth (GT) kommt verz√∂gert und kann sich √ºber die Zeit noch ver√§ndern

<img src="img/ground-truth-zeitstrahl.png">    

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* _Ein lauff√§higes ML System_
* Betrieb
* Abschluss / R√ºckblick

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Phasen eines ML Projekts

<img src="img/ml-workflow/6.PNG" class="fragment">

</textarea>
</section>

	<section data-markdown class="fragments">
### Phase I: Exploration

* in der ersten Phase eines Machine Learning Projekts wird die Anwendungsidee validiert und ein
funktionsf√§higes Modell entwickelt.
* dabei ist ein schnelles iterieren und ausprobieren von Ideen zentral
* das Ziel ist *nicht* ein sinnvolles St√ºck Software
* Scripting passt hier besser als Programmieren als Ausdruck f√ºr die T√§tigkeit
* das Ziel ist eine schnelle Entwicklung
* Phase 1 endet entweder mit
  * einem funktionsf√§higen Modell mit dem man in Phase II √ºbergeht oder
  * dem Verwerfen des Ansatzes

	</section>

  <section data-markdown style="font-size: xx-large;" class="hands-on">
    <textarea data-template>
## Hands-On 1 - Exploration gemeinsam durchgehen

* Neuronales Netzwerk mit TensorFlow
* 3 Hidden Layers, 100 Neuronen pro Layer
* 1500 Datens√§tze insgesamt
* Training auf allen 6 Features
* Test/Validation auf 300 Datens√§tzen
* Training auf 1200 Datens√§tze
* Normalisierung auf Trainingsdatens√§tzen
* Accuracy Training/Test > 85%

Aus Docker: http://localhost:8888/notebooks/notebooks/exploration.ipynb
<br>
Colab Fallback (falls die Installation noch l√§uft): https://colab.research.google.com/github/openknowledge/mlops-data2day/blob/main/notebooks/exploration.ipynb

  </textarea>
  </section>  

<section data-markdown>
	<textarea data-template>
### Exploration hinterl√§sst gern einen Wust an Notebooks

<img src='img/sketch/notebook-explosion-no-title.png'  style="height: 600px;">

</textarea>
</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Ist ein Wust an Notebooks ein Problem?

* das ist kein Zeichen von einem unprofessionellen Vorgehen
* ergibt sich aus der Arbeitsweise und Zielsetzung
* jeder Experimentator, erprobte ML Ansatz und jede Iteration kann eine neue, komplett entkoppelte Kopie eines Notebooks rechtfertigen
  * "Das Wichtigste in dieser Phase ist die schnelle Iteration" https://twitter.com/marktenenholtz/status/1488134981985583105
  * "Machen Sie ein einfaches Experiment nach dem anderen" https://karpathy.github.io/2019/04/25/recipe/
* nat√ºrlich wird dabei teilweise falsch entkoppelt
  * Wir kopieren alles, auch die Teile, die die beiden Notebooks gr√∂√ütenteils unver√§ndert teilen
  * Solange wir aber nicht wissen was die relevant gemeinsamen Teile sind m√ºssen wir damit weiter machen
* welcher Ansatz mehr Liebe verdient wird erst am Ende dieser Phase klar
</textarea>
	</section>

	<section data-markdown class="fragments">
### Phase II: Professionalisierung

* in der zweiten Phase wird die skizzierte L√∂sung in ein langlebiges Projekt umgewandelt
* alle Regeln einer guten Software-Entwicklung gelten von nun an
* Stabilit√§t und Funktionalit√§t wird gew√§hrleistet
* Die Rahmenbedingungen der Produktionsumgebung m√ºssen erf√ºllt werden
  * Art des Deployments, Sprache, Latenz, Speicher, Bandbreite, etc.
  * Explainability, Bias / Fairness
  * Phase II endet entweder mit
  * reifem Code und Modell mit dem man in Phase III √ºbergeht oder
  * dem Iterieren zur√ºck in Phase I mit neu gewonnenen Erkenntnissen oder falls Rahmenbedingungen nicht erf√ºllt werden

	</section>


	<section data-markdown>
		<textarea data-template>
<img src='img/sketch/ml-dev-prozess.png' style="height: 650px;">
</textarea>
	</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Was geh√∂rt in Bibliotheken ausgelagert?

* Wir k√∂nnen nicht mit einem Notebook in Produktion gehen
  * also muss alles was wir in Produktion brauchen aus den Notebooks herausgezogen werden
* Bestimmte Teile eines Notebooks haben sich als stabil herausgestellt und sollten nicht bei jeder Kopie entkoppelt werden
* Alles was sich nach Software anf√ºhlt (Klassen, Funktionen, etc.) ist auch Software
  * diese Teile sollten auch wie solche behandelt werden  
* Professionalisierung muss oft gut abh√§ngen
  * Es stellt sich erst langsam heraus, was in ein Modul geh√∂rt
  * erste Version der extrahierten Module ist mit Sicherheit nicht endg√ºltig
</textarea>
	</section>

<section data-markdown>
### Scripte	

* Manche Notebooks werden in Phase II zu Skripten, die eine d√ºnne API um die Module sind. 
* K√∂nnen auch in CI/CD eingebaut werden.

</section>

<!-- <section data-markdown>
	<textarea data-template>
### CI/CD bei klassischem DevOps

<img src="img/ci-cd-flow-desktop.png" style="height: 100%">

https://www.redhat.com/en/topics/devops/what-is-ci-cd

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### DevOps vs MLOps

<img src="img/devops-vs-mlops.png" style="height: 600px">
</textarea>
</section>
 -->
<section data-markdown class="hands-on">
	<textarea data-template>
## Hands-On IIa - wie wollen wir das Modell validieren?

* bei automatisiertem Training k√∂nnen wir ein erfolgreiches Training nicht mehr manuell √ºberwachen
* wie k√∂nnten automatisierte Tests auf dem Modell aussehen?
* Diskutiert mit euren Nachbarn und schreibt eure Vorschl√§ge auf
* Inspiration: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45742.pdf

</textarea>
</section>

<section data-markdown class="hands-on" style="font-size: x-large;">
	<textarea data-template>
## Hands-On IIb - Scripte, manuelles CI/CD

_Wir f√ºhren unsere Pipeline aus und sehen uns gemeinsam die Scripte an_

1. √ñffne ein Terminal aus deinem Notebook Server heraus (New->Terminal)
1. `cd scripts`
1. Modell trainieren: `./train.py -d ../data/reference.csv -m classifier`
   <!-- * Windows: `python train.py -d ..\data\reference.csv -m classifier` -->
1. Modell validieren: `./validate.py -d ../data/reference.csv -m classifier`
   <!-- * Windows: `python validate.py -d ..\data\reference.csv -m classifier` -->
1. Deploy: Modell in `/model` Ordner schieben

*Aufgabe*
1. Sieh dir die Scripte √ºber die IDE oder den Notebook Server an
1. Welche Validierungen werden verwendet?
1. Passe ggf. die Werte an, sodass alle Scripte durchlaufen
1. Erg√§nz das Validation Script mit euren Validierungen, wenn sie einfach umzusetzen sind
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauff√§higes ML System
* _Betrieb / Monitoring_
* Abschluss / R√ºckblick

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
<img src='img/everybody-gansta.png' style="height: 600px">

<small>

https://twitter.com/karpathy/status/1486215976559398915
</small>

</textarea>
</section>


	<section data-markdown class="fragments">
### Phase III: Produktion / Betrieb

in der dritten Phase wird die L√∂sung in Betrieb genommen

* alle Regeln des produktiven Einsatzes von Software gelten auch hier
* Monitoring hat zus√§tzliche Herausforderungen
  * Natur und Verteilung der Anfragen und auch Vorhersagen muss permanent √ºberwacht werden
* Phase III endet entweder mit
  * der Abschaltung 
    * entweder bald weil nutzlos oder
    * sp√§ter weil durch neues System ersetzt
  * dem Iterieren zur√ºck in Phase II mit neu gewonnenen Erkenntnissen
  * dem Iterieren zur√ºck in Phase I mit neu gewonnenen Erkenntnissen oder einem Neuansatz (h√§ufig ebenfalls ein Zeichen f√ºr einen Fehlschlag)

			</section>

	<section data-markdown>
		<textarea data-template>
### Nicht aller Code ist f√ºr Produktion gedacht		

* Was geht in Produktion
  * Vorhersage
  * Monitoring

* Was geht nicht in Produktion
  * Training
  * Validierungen
  * Analytics
  * Visualisierung
</textarea>
	</section>

  <section data-markdown>
    <textarea data-template>
  ### Wie kann man ML in Produktion bringen?
  
  <img src="img/mlops/Micro-ML.png">
  </textarea>
  </section>
    
	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIa - Produktionsumgebung

1. Im Ordner `app` setzen wir den Rating Service komplett mit Adapter um
1. In `app.py` ist ein einfacher Flask Server implementiert
1. Der Server wird bereits √ºber Docker compose gestartet
1. Checken, ob der Service l√§uft: http://localhost:8080/ping
1. √úber http://localhost:8000/client.html simulieren wir den orchestrierenden/nutzenden Service und machen einfache Requests
1. in `app/client.html` Werte anpassen
   * auf dich als Fahrer
   * au√üerhalb des g√ºltigen Bereichs des Models 
</textarea>
	</section>

  	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIb - Grenzen des Modells

*Diskussion*
* Sollten wir unser Modell f√ºr jede Anfrage verwenden?
* Sollten wir alle g√ºltigen Vorhersagen nutzen?

*Code*
1. `app/app.py` und `app/model_prediction.py` enthalten den Server Code
1. Sieh dir an, wie entschieden wird, ob das ML Modell genommen wird
1. Findest du die Bedingungen sinnvoll?
1. Passe sie deinen Vorstellungen an und probiere das Modell wieder √ºber den Client aus
</textarea>
	</section>


<section data-markdown>
    <textarea data-template>
### Wir erinnern uns: ML Modelle brauchen permanentes Monitoring und Wartung

<img src='img/verfall-2.PNG'>

</textarea>
</section>


	<section data-markdown class="fragments">
		<textarea data-template>
### Woher wei√ü man, dass man ein neues Modell in Produktion braucht?

1. Schon in der Explorationsphase pr√ºfen wie sich das Modell auf neueren Daten verh√§lt
   * wie schnell degradiert die Performance?
   * Mindestens einmal im Jahr, damit man √ºberhaupt noch wei√ü wie es geht
1. Wenn die Metrik des Modells nachl√§sst in Produktion
   * Daf√ºr braucht man die Ground Truth der Daten aus Produktion
   * Manchmal bekommt man diese unmittelbar nach der Vorhersage durch die Reaktion eines menschlichen Benutzers
   * Oft aber auch erst nach nennenswerter Verz√∂gerung 
1. *Wenn sich die Verteilung der Daten der Anfragen oder Vorhersagen deutlich von denen des Trainings unterscheiden* 

</textarea>
	</section>

<section data-markdown>
	<textarea data-template>
### Monitoring mit Evidently, Prometheus und Grafana

<img src="img/mlops/evidently_grafana_service.png">

<small>https://evidentlyai.com/blog/evidently-and-grafana-ml-monitoring-live-dashboards
<br>
https://docs.evidentlyai.com/integrations/evidently-and-grafana
</small>

</textarea>
</section>

<section data-markdown>
### Wir setzen einen eigenen Monitoring-Server auf

* Basiert auf Prometheus, Grafana und Evidently
* Code basiert auf https://github.com/evidentlyai/evidently/tree/main/examples/integrations/grafana_monitoring_service
* Nicht direkt Teil des eigentlichen Prod-Servers
* Zus√§tzliche Requests werden gegen den Monitoring-Server gemacht
* Requests √ºber die Zeit verteilt werden simuliert
</section>
	
<section data-markdown>
### Prometheus

Metric Server

* https://prometheus.io/
* https://prometheus.io/docs/prometheus/latest/getting_started/
  * https://prometheus.io/download/

</section>

<section data-markdown>
### Grafana

Visualisierung mit Dashboards
	
https://play.grafana.org/
</section>

<section data-markdown>
### Evidently

ML Performance Monitoring
	
https://evidentlyai.com/
</section>

<section data-markdown>
	<textarea data-template>
### Drift-Erkennung mit Prometheus, Evidently und Grafana

<img src="img/mlops/grafana-evidently-drift.png">

https://docs.evidentlyai.com/reports/data-drift
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wie erkennen wir Drift?

* Es wird ein Statistischer Test auf den Eingabe-Daten ausgef√ºhrt
* Unsere Features sind numerisch und kategorisch
  * in `metrics_app/config.yaml` festgelegt
* Die Anfragen in Production werden verglichen mit Referenz-Datensatz, den wir zum Training benutzt haben (`datasets/insurance`)
* Evidently sucht als Default eine passende Metrik aus, es muss also nicht unser Problem sein
* Man kann aber auch von Hand konfigurieren, sowohl Test als auch Parameter
  * https://docs.evidentlyai.com/user-guide/customization/options-for-statistical-tests
  * https://docs.evidentlyai.com/user-guide/customization/options-for-data-target-drift

https://docs.evidentlyai.com/reports/data-drift

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Welcher Statistischer Test / welche Metrik?

es gibt leider nicht den einen passenden Test

* manche passen nur gut f√ºr kleine (< 1000) Datenmengen
  * unsere Datenmengen sind gr√∂√üer als 1000
* manche k√∂nnen nicht nur auf numerischen sondern kategorischen Daten arbeiten
  * wir brauchen beides
* manche sind zwischen 0 und 1 normiert
  * das ist uns eher egal
* unsere Metriken
  * Wasserstein Metrik f√ºr numerische Daten
  * Jensen-Shannon Distanz f√ºr kategorische Daten  

https://evidentlyai.com/blog/data-drift-detection-large-datasets
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Die Wasserstein-Metrik

_Wenn jede Verteilung als ein Haufen von ‚ÄûErde‚Äú angeh√§uft auf dem metrischen Raum betrachtet wird, dann beschreibt diese
Metrik die minimalen ‚ÄûKosten‚Äú der Umwandlung eines Haufens in den anderen._

* nicht zu sensitiv, zeigt nur gr√∂√üere Ver√§nderungen an
* normiert in Ver√§nderungen in Standardabweichungen
* kann (offensichtlich) √ºber 1 gehen
* ab 0.1 gehen wir von einem Drift aus
* funktioniert nur f√ºr numerische Daten

https://de.wikipedia.org/wiki/Wasserstein-Metrik
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Jensen-Shannon Distanz

* Jensen-Shannon Distanz ist wie Wurzel aus der Divergenz
* zwischen 0 und 1
* ab 0.1 gehen wir von einem Drift aus
* funktioniert auch f√ºr kategorische Daten
* basiert auf Kullback‚ÄìLeibler Divergenz, relative Entropie
* Histogramme werden verglichen, Gr√∂√üe des Samples daher egal
* Binning f√ºr kategorische Daten offensichtlich
* Intuition: wie viel Information/Entropie/√úberraschung steckt im Unterschied der beiden Verteilungen?


https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence
https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
</textarea>
</section>

<!-- <section data-markdown class="fragments">
	<textarea data-template>
### P-Werte

* die statistische Verteilung der jeweiligen Features
* weicht diese im Prod signifikant von der Verteilung im Training ab?
* diese Abweichung wird √ºber eine Metrik berechnet
* es kommt eine Konfidenz heraus, ob die Verteilungen unterschiedlich sind
* ab (default) 95% Konfidenz geht man von einer Abweichung aus 
* das bedeutet, dass es eine 5% Wahrscheinlichkeit gibt, dass die Verteilungen eigentlich doch gleich sind, man es aber nur gerade schlechte Beispiele sieht

https://de.wikipedia.org/wiki/P-Wert
</textarea>
</section> -->

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV - Monitoring und Sinn daraus machen

### Teil I: Services checken 

* Monitoring App, Metrics aus Evidently: http://localhost:8085/metrics
* Prometheus: http://localhost:9090
* Grafana: http://localhost:3000

</textarea>
	</section>

  <section data-markdown>
    <textarea data-template>
### Unsere Zeitreise beginnt
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Produktion simulieren

* wir simulieren 3 Jahre Betrieb
* jeder Monat hat 1500 Datens√§tze
* insgesamt 36 Monate, 54.000 Datens√§tze
  </textarea>
</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV - Monitoring und Sinn daraus machen

### Teil II: Request simulieren

1. Einloggen in Grafana: http://localhost:3000
   1. admin/admin
1. √ñffne ein Terminal aus deinem Notebook Server heraus (New->Terminal) oder nutze dein bestehendes
1. Mit `./scripts/example_run_request.py -e evidently_service` Anfragen simulieren
1. Das Dashboard `Evidently Data Drift Dashboard` aufrufen
1. Darin den Datensatz `insurance` ausw√§hlen (dies kann eine Minute dauern)
1. Stelle sicher, dass sich das Dashboard aktualisiert
1. Was beobachtest du? Welche Features driften? Wie kann man das erkl√§ren?

</textarea>
	</section>

  <section data-markdown>
  <textarea data-template>
### Manche Features driften

<img src="img/feature-drift.png">
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Was ist passiert?

* die Performance des Modells degradiert wahrscheinlich
* aber wir haben erst nach Jahren eine Ground Truth, die uns das anhand der Metrik zeigt
* als Ersatzmetrik nehmen wir Feature drift
  * miles: extremer Drift
  * age: signifikanter Drift
  * emergency_braking: signifikanter Drift

  </textarea>
</section>

<section data-markdown>
## Alarm bei Data Drift

* Ver√§ndert sich die Art der Eingabedaten?
  * Oft ausgedr√ºckt durch die Verteilung der einzelnen Features

* Muss nicht notwendig schlimm sein
  * Wenn entscheidende Features betroffen sind, kann es ein Problem sein 
* Alarm anhand von
  * Wichtigkeit der driftenden Features
  * Anzahl der driftenden Features
  * Ausma√ü des Drifts
<!-- * Daf√ºr wichtig
  * Vern√ºnftige Tests anhand der Metriken
  * Vern√ºnftige Konfidenzintervalle -->

</section>

<section data-markdown>
### Drift braucht Interpretation
  
1. Leute werden immer √Ñlter, das passiert aber langsam (age)
1. Es wird immer weniger Auto gefahren, Leute steigen um auf die Bahn und √∂ffentliche Verkehrsmittel (miles)
1. Die Sicherheit der Autos wird immer besser und der Einfluss der individuellen Fahrleistung wird verringert (emergency_braking, pred) 
 
</textarea>
</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV - Monitoring und Sinn daraus machen

### Teil III: Einen passenden Grafana Alert erzeugen

1. Leg einen Ordner an, in dem du sp√§ter deinen Alert speicherst: http://localhost:3000/dashboards/folder/new
1. Einen Alert kann man in Grafana als Regel oder direkt auf einem Dashboard einrichten
1. Editiere im Dashboard das Panel f√ºr den Anteil driftender Features
1. Im Alert Tab kannst die Bedingung f√ºr den Alarm festlegen
1. Nach dem Speichern sollte ein farbiges Herz auf dem Panel den Status angeben
1. Stelle sicher, dass dein Alert ausgel√∂st wird
1. Im Alerting Menu k√∂nnen Contact Points zur Benachrichtigung angelegt werden

https://grafana.com/docs/grafana/latest/alerting/

</textarea>
	</section>


    <section data-markdown>
  <textarea data-template>
### Drift √ºber die Zeit

<img src="img/tw-drift-3.png">
  </textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Erster Test: M√ºssen wir auf den Drift reagieren?

*Es k√∂nnte sein, dass unser Modell flexibel genug ist*  

1. `./validate.py -d ../data/month-12.csv -m classifier`
1. Performance des Modells ist tats√§chlich stark zur√ºck gegangen
1. Unsere Interpretation des Drifts war eine gute Ersatzmetrik
</textarea>
</section>


<section data-markdown>
## Ma√ünahmen bei Drift

* *Neue Version des Modells trainieren*
  * Neue Daten aufnehmen (und labeln)
  * Neue Features erzeugen
* Schnelle Ma√ünahme
  * Pre-/Post-Processing des Modells neu kalibrieren
  * Schwellwerte f√ºr Anwendung anpassen
  * Bestimmte Bereiche ausklammern 
  * Modell Architektur √§ndern (oder fixen) und neu trainieren
* Sehr schnelle Ma√ünahme: Fallback
  * Manuell
  * Heuristik / Baseline
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wir gehen zur√ºck in die Phasen I und II

* hier wird wieder in Notebooks gearbeitet
* die Bibliotheken werden inkludiert und bei jeder √Ñnderung neu geladen
* Jupyter Lab bietet eine gemeine Oberfl√§che f√ºr Notebooks und Bibliotheken
* Eine Kombination von Visual Studio Code und Jupyer Notebooks ist ebenso m√∂glich
* R√ºckkehr in Phase I muss nicht radikal sein
  * wenn Ansatz vergleichbar kann Phase II so erhalten bleiben
  * neue Ergebnisse fliesen dann iterativ in die Professionalisierung ein
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### R√ºckkehr in Phase II: Wie trainiert man neu/nach/weiter?

<img src="img/mlops/stateful-vs-stateless-v2.png">

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Stateful vs Stateless

* Stateless (aka offline)
  * trainiert das Modell periodisch komplett neu
  * verhindert "catastrophic forgetting" (https://en.wikipedia.org/wiki/Catastrophic_interference)
* Stateful (aka online)
  * trainiert permanent weiter mit kleinen Batches
  * eine Art des Transfer Learnings
  * folgt Drifts schnell
  * erfordert zeitnah neue Datenpaare 
  * Online Learning for Recommendations at Grubhub: https://arxiv.org/abs/2107.07106
    * angeblich 45x mal weniger aufw√§ndig als offline
    * 20% bessere Metric als offline

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Beispiel f√ºr Stateless / Offline Training    

1. Wir trainieren weiterhin mit einem einzelnen Monat
1. Wir k√∂nnten auch √§ltere Daten nehmen, aber durch den Drift scheint der neueste Datensatz am erfolgversprechendsten
1. `./train.py -d ../data/month-12.csv -m classifier`
1. Das Training erf√ºllt unsere Anforderungen nicht mehr
1. Entweder
   1. Anforderungen anpassen oder
   1. Zur√ºck an den Zeichentisch (Phase I), z.B.
      1. Model tunen
      1. Andere Features nehmen?
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauff√§higes ML System
* Betrieb
* _Abschluss / R√ºckblick_

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Offene Fragen / Themen
  </textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Zusammenfassung: Arten von Drift

* _Covariate / Input / Data drift_: Verteilung der Eingabe hat sich ge√§ndert
* _Prior / Label / Prediction drift_: Verteilung der Vorhersage hat sich ge√§ndert
* _Concept / Model drift_: Zusammenhang zwischen Eingabe und Vorhersage hat sich ge√§ndert

<img src="https://docs.seldon.io/projects/alibi-detect/en/stable/_images/bg_2d_drift.png" style="height: 100%;">

<small>https://docs.seldon.io/projects/alibi-detect/en/stable/cd/background.html#what-is-drift
</small>
</textarea>
</section>

<section data-markdown style="font-size: x-large;">
	<textarea data-template>
## Drift erfordert Interpretation

Wenn die Welt sich √§ndert, ist Drift zu erwarten und damit ok

|   | Positive Interpretation, keine Ma√ünahme erforderlich  | Negative Interpretation, Ma√ünahme erforderlich  |
|---|---|---|
| *Data und Prediction Drift*  | wichtige Features haben sich ge√§ndert, Modell kommt klar und extrapoliert gut, z.B.: H√∂heres Alter, mehr Risiko  |  wichtige Features haben sich ge√§ndert, Modell extrapoliert nicht sinnvoll |
| *Data aber kein Prediction Drift*  | keine wichtigen Features ge√§ndert, das Modell ist robust genug f√ºr den Drift  | wichtige Features ge√§ndert, Modell extrapoliert nicht sinnvoll |
| *Prediction aber kein Data Drift*  | ???  | wahrscheinlich Concept Drift, neue Analyse der Situation notwendig |
|   |   |   |

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Analyse des Drifts f√ºr Phase I

http://localhost:8888/notebooks/notebooks/analysis.ipynb

Fallback: https://colab.research.google.com/github/openknowledge/mlops-data2day/blob/main/notebooks/analysis.ipynb
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Wahres kausales Modell

Einfluss des Fahrzeugs mit den Jahren deutlich wichtiger geworden 

<img src='img/insurance-causal-model.png' style="height: 300px;">

Kann man in der Realit√§t nicht wissen, muss man modellieren
</textarea>
</section>


<section data-markdown class="fragments">
## Ausblick: Was kann man sonst noch monitoren?

* Qualit√§t der Daten
  * wie ver√§ndern sich fehlende oder falsche Felder
  * Plausibilit√§t
* Daten Drift
  * Verteilung der Eingabedaten
* Prediction Drift   
  * Was gibt das Modell aus?
</section>

<section data-markdown>
## Datenqualit√§t

* Felder
  * fehlen
  * ung√ºltig
  * falsch / unplausibel / Wertebereich verlassen
* Features 
  * konstante (die (meisten) Eingaben haben (fast) denselben Wert)
  * leere
  * fast leer
  * Korrelationen zwischen Features
</section>
	

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Was kann man sonst noch machen

* Outlier-Detection
  * Unser Modell wird nicht extrapolieren k√∂nnen
  * Werte au√üerhalb des Trainings-Bereichs werden wahrscheinlich unrealistisch sicher vorhergesagt
  * Ausrei√üer m√ºssen ohne Ground Truth entdeckt werden 
* Adversarial Detection
  * Bestimmte Eingaben k√∂nnen absichtlich eine grob falsche Vorhersage herbei f√ºhren 
  * Solche Eingaben k√∂nnen erkannt und korrigiert werden
  * Dazu kann z.B. ein Autoencoder benutzt werden, der die Eingabe korrigiert


<small>https://docs.seldon.io/projects/alibi-detect/en/stable/od/methods.html
<br>
https://docs.seldon.io/projects/alibi-detect/en/stable/ad/methods.html
</small>
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Drift Detection f√ºr Bilder

* Input Drift mit Histogrammen von Low-Level-Features (HSV): https://towardsdatascience.com/detecting-semantic-drift-within-image-data-6a59a0e768c6
* Input Drift mit Dimensions-Reduktion: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_ks_cifar10.html 
* Model Drift durch Vergleich mit destilliertem Modell: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_distillation_cifar10.html 
* Ein komplettes Beispiel mit Alibi-Detect: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/alibi_detect_deploy.html#4.-Drift-detection-with-Kolmogorov-Smirnov
* Anomalien in Bildern: https://29a.ch/photo-forensics
* Drift kann auch auf Text festgestellt werden: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_text_imdb.html
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Wann liegt Ground Truth vor?

*Menschliche Experten k√∂nnen die Ground Truth*
* *bestimmen* 
  * sobald ein Mensch die Entscheidungen nachpr√ºft / revidiert
  * bei einem Proposal-System kann das sehr schnell sein
  * bei Dunkelverarbeitung sollte dies in regelm√§√üigen Abst√§nden passieren
* *nicht bestimmen*
  * wir m√ºssen Realit√§ten abwarten
  * bei Zeitreihen wird auf die nahe Zukunft vorhergesagt, sobald diese Eintritt kann √ºberpr√ºft werden
  * oft sind solche Realit√§ten erst nach einiger Zeit wahrnehmbar und unterliegen statistischen Schwankungen (wie bei uns)
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Unsere Vorhersage selbst erzeugt Drift

* Ein Deployment √§ndert das unsere Rolle von Beobachter zu Akteur
* Wir versichern nur Leute mit einer guten Risiko Prognose
  * Wenn nicht, warum sollten wir dann eine √ºberhaupt eine Prognose machen?
* Unsere GT wird mehr und mehr gute Fahrer haben
  * Zumindest ist das unsere Hoffnung (sonst h√§tte die Prognose nicht geklappt)
* Falls nicht (False Negative, Type II Fehler)
  * haben Menschen gelernt, unser System auszutricksen?
  * "Dann versichert eben meine Tochter den Wagen"
* Haben wir gute Fahrer aus verstehen nicht versichert (False Positive, Type I Fehler)
  * M√∂glichkeit: *epsilon-greedy* meistens der Vorhersage glauben, aber manchmal (epsilon) auch einen Fahrer mit schlechter Prognose versichern 
  * Vorhersage mit allen Wahrscheinlichkeiten geht in unsere Datenbank ein

https://twitter.com/ChristophMolnar/status/1569644089724764160
</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
### Zusammenfassung

1. Machine Learning Projekte k√∂nnen in Phasen gedacht werden
1. In der ersten Phase macht man m√∂glichst schnelle Experimente
1. Sollte sich eine Idee als tragf√§hig erweisen, professionalisiert man die Idee
1. Dies ist Voraussetzung und Grundlage f√ºr Produktion
1. In Produktion ergeben sich besondere Herausforderung im Bereich Monitoring
1. Typischerweise m√ºssen Machine Learning Systeme regelm√§√üig nachtrainiert und gepflegt werden
</textarea>
</section>

		<section data-markdown>
			<textarea data-template>
# Vielen Dank

MLOps mit Python

Bleibt gern im Kontakt

Oliver Zeigermann / oliver.zeigermann@openknowledge.de
https://www.linkedin.com/in/oliver-zeigermann-34989773/
https://twitter.com/DJCordhose

Tobias Kurzydym / tobias.kurzydym@openknowledge.de
https://twitter.com/tkurzydym
</textarea>
		</section>

		</div>
	</div>
	<script src="revealjs/reveal.js/dist/reveal.js"></script>
	<script src="revealjs/reveal.js/plugin/notes/notes.js"></script>
	<script src="revealjs/reveal.js/plugin/markdown/markdown.js"></script>
	<script src="revealjs/reveal.js/plugin/highlight/highlight.js"></script>
	<script src="revealjs/config.js"></script>


</body>

</html>