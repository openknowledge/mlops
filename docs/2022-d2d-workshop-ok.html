<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=10.0, user-scalable=yes">

	<title>Workshop: MLOps d2d 2022</title>

	<link rel="stylesheet" href="revealjs/reveal.js/dist/reset.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/reveal.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/theme/white.css" />

    <!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/monokai.css"> -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/zenburn.css"> -->
    <link rel="stylesheet" href="revealjs/highlight-js-github-theme.css" />
    <link rel="stylesheet" href="revealjs/styles.css" />

</head>

<body style="background-color: whitesmoke;">
	<div class="reveal">
		<div class="slides">

<!-- 
Data2Day
https://www.data2day.de/cfp.php

Workshop
https://www.data2day.de/veranstaltung-15183-0-mlops-mit-python.html

9:00 - 16:00

MLOps mit Python

Viele Unternehmen haben die Phase des reinen Experimentierens mit Machine Learning hinter sich gelassen und fragen sich
nun, wie man ein Machine Learning Modell so betreibt wie ein traditionelles Stück Software. Also wie man es in Produktion bringt und dort
erfolgreich hält. MLOps ist der Name für diesen Ansatz. 

In diesem Workshop werden wir uns anhand eines durchgängigen Beispiel durch die Phasen eines Machine Learning Projekts bewegen:
- Exploration: Entwicklung eines Modells mit Jupyter Notebooks und TensorFlow
- Professionalisierung: Refactoring in Module
- Produktion und Monitoring: Produktivsetzung mit Flask und Docker, Monitoring mit Prometheus, Evidently und Grafana

Als ML Werkzeug werden wir TensorFlow nutzen, allerdings ist dies nur ein Beispiel und alles gezeigte gilt auch für alle
anderen Frameworks. Es wird lediglich Erfahrung mit Python und ein Verständnis für ein Machine Learning Projekt
vorausgesetzt. Alles andere wird - soweit nötig - im Workshop eingeführt.

Lernziel:
Teilnehmer sollen anhand eines in sich stimmigen Satzes von Werkzeugen die Konzepte und Ansätze im Bereich MLOps kennen lernen und diskutieren können.

---

Talk: 45 Minuten

https://www.data2day.de/veranstaltung-15048-0-monitoring-von-drift-mit-prometheus-grafana-und-evidently.html

Monitoring von Drift mit Prometheus, Grafana und Evidently

Machine-Learning-Modelle erfordern besondere Maßnahmen beim Monitoring. Die Vorhersagekraft des Modells ist dabei
besonders wichtig, aber oft nicht direkt beobachtbar.

In diesem Live-Demo ohne Folien sehen wir uns eine Machine Learning Anwendung an, die mit einem bestimmten Modell in
Produktion gegangen ist. Das Model basiert auf TensorFlow und wir mit einem Flask über Docker serviert. In simulierten
Anfragen detektieren wir einen Drift, der einen Alarm in Grafana auslöst. Diesen interpretieren wir und entscheiden, ob
er kritisch und was die passende Maßnahme ist.


Die Teilnehmenden bekommen einen Eindruck von einem realistischen Setup für das Monitoring von Drift. Es wird erklärt,
warum das so wichtig ist und wie man eine passende Maßnahme ableitet

---

Aus: https://arxiv.org/abs/2007.06299
The machine learning lifecycle extends beyond the deployment stage. Monitoring deployed models is crucial for 
continued provision of high quality machine learning enabled services. 
Key areas include model performance and data monitoring, detecting outliers and data drift using statistical techniques, 
and providing explanations of historic predictions. We discuss the challenges to successful implementation of solutions 
in each of these areas with some recent examples of production ready solutions using open source tools.


---

OOP

Titel: MLOps - wie bringt man ein Machine Learning Modell in Produktion und hält es dort?

MLOps beschäftigt sich mit dem Thema, ein Machine Learning Modell in Produktion zu bringen und es dort erfolgreich zu betreiben.

Ein Machine Learning Modell läuft fast immer als Teil eines komplexeren Software Systems. 
Und so ist die die erste Herausforderung ist, das vermeintlich fertige Modell in in ein solches System zu integrieren.
Für die Produktion müssen Metriken erarbeitet werden, die die Performanz des Modells auch in Zukunft bewerten können.
Da es häufig (zumindest zeitnah) keine gesicherten Erkenntnisse über die Qualität der Vorhersage gibt, müssen 
Platzhalter dafür gefunden werden.
Ebenso muss überlegt werden, wie man die Ursache für die Verschlechterung der Vorhersagequalität eigentlich gemacht werden sollte und wie man 

die für einzelne Beispiele 

Sobald das Modell in Produktion ist, muss es auf eine 




https://en.wikipedia.org/wiki/MLOps


---

https://qconsf.com/track/oct2022/mlops



Talk: Drift Detection in MLOps

MLOps deals with the topic of bringing a machine learning model into production and keeping it there.



problem of detecting and handling drift in machine learning models.

Bringing a machine learning model into production and making it stay there is a complex task.

Once your model is in production you need to monitor it for degrading performance. 
Since many applications do not provide immediate feedback (or none at all) on the prediction quality, 

you need to provide a way to

you need to monitor it for drift.

In this talk I will look at existing tools and approaches independent of any solution provider from the perspective of a practitioner.


In this talk we will look at https://evidentlyai.com/ and https://github.com/SeldonIO/alibi-detect

Once a drift has been detected, we will use the alibi-detect library to provide a detailed explanation of the drift.

Eventually, we will act on the explanation and provide a solution based on serverity and the context of the drift.


Oliver Zeigermann is Head Of AI at Open Knowledge, a leading provider of machine learning services. 
He is an O’Reilly book author and the author of a Manning video course. Oliver has been a speaker at various conferences 
including ODSC West, TensorFlow world, and MLConf. 
His main interest is in the intersection between traditional software engineering and machine learning.

What would be the 3-4 main actionable takeaways you would like your audience to remember and to use after your talk?
1. DevOps for machine learning has special requirements
2. Drift of various types is a problem not only of machine learning, but all heuristic business code
3. Detecting, evaluating and acting upon drift is the most critical part of the machine learning lifecycle 


---

MlOps - concepts, not tools

Artificial intelligence is no magic and machine learning is just an alternative way of developing software. Thus most of what applies to the traditional way of writing software also is valid for machine learning projects.

This is a challenge since data scientists are often not familiar with basic concepts of software engineering
like refactoring, versioning, continuous integration, and automated testing. On the other hand traditional software developers often lack the
knowledge of the special requirements of a machine learning project.

In this talk I will argue that tools alone can not solve MLOps,
since the awareness of the challenges both for data science and software development as a combination need to be addressed first.
What process should be followed and what tasks need to be performed at each step is the main topic of this talk.

kross@fortiss.org

"Fit a model on a given dataset sampled from a distribution that represents our problem" could be a casual way of describing the classic idea of machine learning.
 
This, however, does not reflect the challenges of many modern machine learning projects. Data does not simply exist and the sample people are working on often is not static, but rather gets generated incrementally in smaller batches. Also, the distribution the data is sampled from can change with the domain it comes from.

In this talk we will address the following questions:
- Workflow: What kind of workflow reflects a more incremental take on machine learning?
- Training for Production: How can we anticipate if our model works well in production and when to retrain?
- Explainability: How can we find out how our model works to check for bias and analysis of degradation?
- Unwanted Bias: Data sampled in the past is used to predict the future. How can we find out if our model has bias that is (no longer) wanted?
- Testing: What is a proper way of testing a model to find regressions from previous cycles?
- Monitoring: How do we detect when our model actually degrades in production and how do we react?


Oliver Zeigermann is Head Of AI at open knowledge GmbH, a leading provider of software development services. 
He is an O’Reilly book author and the author of a Manning video course. Oliver has been a speaker at various conferences 
including ODSC, TensorFlow world, and MLConf. His main interest is in the intersection between traditional software engineering and machine learning.

-->

<section data-markdown>
	<textarea data-template>
### Bevor es los geht

1. WLAN: xxx
1. Diese Folien: https://bit.ly/d2d-2022-mlops
1. Falls noch nicht geschehen, bitte das Projekt installieren
   1. Python Umgebung installieren (z.B. Miniconda, Achtung M1): https://docs.conda.io/en/latest/miniconda.html
   1. Git LFS installieren: https://git-lfs.github.com/
   1. Projekt klonen: https://github.com/openknowledge/mlops-data2day
   1. Abhängigkeiten über Conda installieren: https://github.com/openknowledge/mlops-data2day#local-installation-for-development
1. Nachher gibt es noch Zeit und es gibt auch einen Fallback

_Bei Problemen den Nachbar oder Tobias und Olli fragen_
</textarea>
</section>

<!-- 
MLOps mit Python

https://www.data2day.de/veranstaltung-15183-0-mlops-mit-python.html

Agenda:

09:00 Uhr: Beginn

Einführung
- MLOps: Phasen und Workflow in einem ML Projekt
- Daten entstehen über die Zeit
- Unser Fallbeispiel und Datenbasis

10:45 - 11:00 Uhr: Kaffeepause

Ein lauffähiges ML System
- Installation und Start des ML Systems
  - Docker
  - Flask
- wichtige Metriken
- Was kann man wie testen? 

12:30 - 13:30 Uhr: Mittagspause

Betrieb
* Benutzung des Modells simulieren
* Drifts, Monitoring, Alerting
* Evidently, Prometheus, Grafana
* Bewertung von Drift

15:00 - 15:15 Uhr: Kaffeepause

Abschluss / Rückblick
* Ausblick: was passiert nach Produktivsetzung?
* Offene Fragen

ca. 16:00 Uhr: Ende


TECHNISCHE ANFORDERUNGEN
Falls ihr ein Gerät eurer Firma verwendet, überprüft vorher bitte, ob eines der folgenden, gelegentlich vorkommenden Probleme bei euch auftreten könnte.
Workshop-Teilnehmer:in hat keine Administrator-Rechte.
Corporate Laptops mit übermäßig penibler Sicherheitssoftware
Gesetzte Corporate-Proxies, über die man in der Firma kommunizieren muss, die aber in einer anderen Umgebung entsprechend nicht erreicht werden.

Vorbereitung
Dieser Workshop geht davon aus, dass du bereits eine Python-Entwicklungsumgebung mit IDE, Python Distribution und Git 
auf deinem Rechner lauffähig hast.
Sollte das nicht der Fall sein und du nicht sicher bist, welche Software passt, installiere bitte:
* Anaconda: https://www.anaconda.com/products/individual
* Visual Studio Code: https://code.visualstudio.com/
* Git: https://git-scm.com/downloads
* Docker: https://docs.docker.com/get-docker/

Wir werden auf einem Beispielprojekt arbeiten. Hab bitte bereits vor dem Workshop
das Projekt https://github.com/openknowledge/mlops-data2day geklont und wie im Readme beschrieben installiert.
 -->

<!-- <section data-markdown class="hide" style="font-size: xx-small">
	<textarea data-template>
### Story (auch für Talk mit Schwerpunkt auf hinteren Teil)

Orchestrierung hier händisch, es geht um das Konzept und welches Tool das richtige ist hängt vom Projekt und der Umgebung ab.

1. Problemstellung: innovative Kfz-Versicherungsgesellschaft
1. Notebook Server Starten
1. Exploration durchgehen: http://localhost:8888/notebooks/notebooks/exploration.ipynb
   1. Features
   1. Was wollen wir vorhersagen?
1. Überblick über die 3 Phasen, wieso kann ich damit nicht in Prod gehen
1. Professionalisierung: Notebook in Libs und Scripte
1. Build: Scripte manuell laufen lassen
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./train.py -d ../data/reference.csv -m classifier`
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./validate.py -d ../data/reference.csv -m classifier`
   1. Modell in app Ordner schieben
   1. Diskussion: Modell und Daten sind groß oder Binary, versionieren, aber nicht in Git
1. Produktion
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/app$ ./app.py`
   1. `http://localhost:8080/ping`
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day$ python -m http.server`
   1. `http://localhost:8000/app/client.html`
   1. in `app/client.html` Werte anpassen und herumspielen damit
1. Produktion in Docker
   1. Notwendig: Volumes von Prometheus und Grafana platt machen, um von 0 zu starten???
   1. `(base) olli@DESKTOP-BEN73DP:~/mlops-data2day$ docker compose up --build`
   1. `http://localhost:8085/metrics`
   1. `http://localhost:9090`
   1. `http://localhost:3000/`
1. Produktion simulieren
   1. Story:
      1. die Performance des Modells degradiert
	  1. aber wir haben erst nach Jahren eine Ground Truth, die uns das anhand der Metrik zeigt
	  1. wir simulieren 3 Jahre Betrieb mit
         1. Leute werden immer Älter, das passiert aber langsam (age)
	     1. Es wird immer weniger Auto gefahren, Leute steigen um auf die Bahn und öffentliche Verkehrsmittel (miles)
	     1. Die Sicherheit der Autos wird immer besser und der Einfluss der individuellen Fahrleistung wird verringert (emergency_braking, pred) 
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day$ ./scripts/example_run_request.py` 
   1. `http://localhost:8085/metrics`
   1. `http://localhost:3000/d/U54hsxv7k/evidently-data-drift-dashboard?orgId=1&refresh=5s`
1. Analyse   
  1. Drift ist offensichtlich, aber gibt es überhaupt ein Problem?
  1. Letztlich bekommen wir GT rein von Daten, die 2 Jahre alt sind
  1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./validate.py -d ../data/month-12.csv -m classifier`
  1. Performance des Modells ist stark zurück gegangen
  1. Wir sollten neu trainieren: zurück zu Phase II 
  1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/scripts$ ./train.py -d ../data/month-12.csv -m classifier`
  1. Neues Modell erfüllt nicht mehr unsere Anforderungen: zurück zu Phase I
     1. http://localhost:8888/notebooks/notebooks/analysis.ipynb
     1. Anforderungen ändern?
	 1. Modell tunen?
</textarea>
</section>
 -->

 <section data-markdown class="preparation">
# Model Karte / Architektur

* Schritt für Schritt anlegen und diskutieren
* Was sind die Architektur-Entscheidungen?

* Bisschen an das angelehnt: https://huggingface.co/docs/hub/models-cards
</section>


			<section data-markdown>
				<textarea data-template>
# MLOps mit Python

data2day 2022, https://www.data2day.de/veranstaltung-15183-0-mlops-mit-python.html

Tobias Kurzydym / tobias.kurzydym@openknowledge.de

Oliver Zeigermann / oliver.zeigermann@openknowledge.de

Folien: https://bit.ly/d2d-2022-mlops
<!-- Folien: https://openknowledge.github.io/mlops-data2day/2022-d2d-workshop.html -->
    </textarea>
			</section>

<section data-markdown>
  <textarea data-template>
### MLOps

* KI ist keine Magie
* KI ist eher "künftig Informatik"
* Machine Learning is die zur Zeit wichtigste Technik im Bereich KI
* Machine Learning ist eine alternative Art, Software zu entwickeln
* Daher sollten wir die bekannten Techniken der Software Entwicklung auf Machine Learning anwenden

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Spezielle Anforderungen an die Entwicklung von ML Anwendungen

* Modelle so zähmen / trainieren, dass sie überhaupt funktionieren
* Debuggen
* In-/Output kodieren
* Fallback für invalide Input- oder Output-Bereiche
* Automatisierte Tests

</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Tobias

<img src='img/Tobias_Kurzydym_3.jpg'>

<p>
<a target="_blank" href="mailto:tobias.kurzydym@openknowledge.de">Tobias Kurzydym</a>:
Dev
</p>    
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Olli

<div style="display: flex;">
<div style="flex: 50%;">
  <a href='https://oreilly.de/produkt/machine-learning-kurz-gut-2/'>
  <img src='img/ml-buch-v2.jpg' height="400">
  </a>
</div>
<div style="flex: 50%; font-size: x-large;">
  <img src='img/olli-opa.jpeg'>
</div>
</div>
<p>
<a target="_blank" href="mailto:oliver.zeigermann@openknowledge.de">Oliver Zeigermann</a>:
Head of AI@OpenKnowledge
</p>    
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Wer seid ihr?

* Was macht ihr?
* Was wisst ihr schon?
* Warum seid ihr hier?
</textarea>
</section>

	<section data-markdown>
		<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauffähiges ML System
* Betrieb
* Abschluss / Rückblick

</textarea>
	</section>

	<section data-markdown>
		<textarea data-template>
# Agenda

* _Grundlagen MLOps, Unser Beispiel_
* Ein lauffähiges ML System
* Betrieb
* Abschluss / Rückblick

</textarea>
	</section>


	<!-- <section data-markdown>
		<textarea data-template>
## MLOps: Phasen und Workflow in einem ML Projekt

ML Projekte sind speziell
</textarea>
</section> -->

<section data-markdown class="fragments">
### Was ist MLOps?

* MLOps ist abgeleitet von DevOps
* Durch MLOps kommt ML in Produktion und wird in Betrieb gehalten
* Dazu kommen eine Reihe von Werkzeugen und Praktiken zum Einsatz
* Überschneidung aus
  * Softwareentwicklung
  * Operations
  * Data Science
</section>

<section data-markdown class="fragments">
### Warum MLOps?

* im akademischen Leben zählt für einen Wettbewerb häufig nur der Score (Güte) des Modells
* dieser Ansatz hat sich im Bereich des Data Science auch in der Praxis breit gemacht
* die Praxis ist aber keine Kaggle Competition
* In-Sample Evaluation sagt nur bedingt etwas für Eignung in
einer praktischen Anwendung aus
* Out-Of-Sample Evaluation häufig erst im produktiven Betrieb möglich (evtl. nur mitlaufen lassen)
</section>

<!-- <section data-markdown>
	<textarea data-template>
### MLOps @ Gitlab

* For traditional software, logic is made explicit through code, while on for ML the logic is implicit in the data, and extracted through a variety of technique's.
* Note that MLOps is not a branch of DevOps, it's a superset of it.

 https://gitlab.com/gitlab-org/incubation-engineering/mlops/meta/-/issues/58
</textarea>
</section>
 -->

 <section data-markdown>
	<textarea data-template>
_Nothing is static, everything changes. But some things change faster than others. Culture changes slowly. Human nature is even slower._

https://twitter.com/fchollet/status/1540548737302335490
<br>
https://de.wikipedia.org/wiki/Panta_rhei
</textarea>
</section>

 <section data-markdown>
  <textarea data-template>
### Machine Learning Anwendungen brauchen Wartung

<img src='img/verfall-1.PNG'>

Das gilt nicht nur für ML Anwendungen, aber bei diesen ist es offensichtlicher
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Machine Learning Anwendungen brauchen Wartung

<img src='img/verfall-2.PNG'>

Das gilt nicht nur für ML Anwendungen, aber bei diesen ist es offensichtlicher
</textarea>
</section>


<!-- <section data-markdown>
	<textarea data-template>
Fear of deploys is the largest source of technical debt and wasted time in most engineering orgs.

https://twitter.com/mipsytipsy/status/1530664380961943553 
</textarea>
</section>


	<section data-markdown>
		<textarea data-template>
		
Hidden Technical Dept: https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf
<br>
https://developers.google.com/machine-learning/guides/rules-of-ml
</textarea>
</section>



<section data-markdown>
	<textarea data-template>
### Realistischer als mir lieb ist

By far, the most accurate representation of machine learning pipelines in the real world 🍿 😀

https://twitter.com/bindureddy/status/1552073812157403136
</textarea>
</section>

<section data-markdown>
	<textarea data-template>

Honestly: sometimes I feel defeated because ML observability is so hard. All facets are hard -- detecting, diagnosing, reacting to bugs. We don't have realtime ground truth labels (except recsys) so we don't know asap when performance goes down. Lots of $$ left on the table (1/6)

https://twitter.com/sh_reya/status/1539420163480489984? 
</textarea>
</section>
 -->


<section data-markdown>
### MLOps Is a Mess But That's to be Expected

* MLOps today is in a very messy state with regards to tooling, practices, and standards. 
* However, this is to be expected given that we are still in the early phases of broader enterprise machine learning adoption. 
* As this transformation continues over the coming years, expect the dust to settle while ML-driven value becomes more widespread.

https://www.mihaileric.com/posts/mlops-is-a-mess/
</section>
		
	
<section data-markdown>
    <textarea data-template>
## Die Tool-Landschaft ist divers und meist (noch) Inhouse

<img src='img/mlops/mltools-ih.jpg' style="height: 400px;">

<small>

* https://towardsdatascience.com/lessons-on-ml-platforms-from-netflix-doordash-spotify-and-more-f455400115c7
* Diskussion: 
  * https://twitter.com/adamlaiacano/status/1458124198166122503
  * https://twitter.com/rahulj51/status/1455431014671699971

</small>

</textarea>
    </section>

<section data-markdown class="fragments">
### Ansatz des Workshops

* Der Bereich MLOps ist bisher weder im Bereich Framework noch konzeptionell standardisiert
* Wir spezialisieren uns nicht auf eine Art der Workflow Ausführung
* Für diesen Workshop sind wir selbst der Orchestrator und führen die einzelnen Schritte händisch aus

</section>

<section data-markdown style="font-size: x-large;" class="hands-on">
	<textarea data-template>
## Hands-On 0 - Lokale Arbeitsumgebung überprüfen oder herstellen

Helft bitte euren Nachbarn, falls eure Installation schon läuft

1. Wir empfehlen: arbeitet zusammen mit euren direkten Nachbarn
1. Stellt sicher, dass zumindest einer eurer Rechner eine lauffähige Installation hat 
1. Sagt kurz Hallo
1. Falls noch nicht installiert
   1. Python Umgebung installieren (z.B. Miniconda, Achtung M1): https://docs.conda.io/en/latest/miniconda.html
   1. Git LFS installieren: https://git-lfs.github.com/
   1. Projekt klonen: https://github.com/openknowledge/mlops-data2day
   1. Die Abhängigkeiten des Projekts über Conda installieren wie hier beschrieben: https://github.com/openknowledge/mlops-data2day#local-installation-for-development
1. `jupyter notebook`
1. Click auf den angezeigten Link und navigiere zu `notebooks/exploration.ipynb` (http://localhost:8888/notebooks/notebooks/exploration.ipynb)

</textarea>
</section>

<section data-markdown>
	<textarea data-template>

## Unser Fallbeispiel und Datenbasis

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Unser Beispiel: Vorhersage von Risiken

* Wir sind CTO einer hochinnovativen Kfz-Versicherungsgesellschaft
* Anders als andere Versicherungsgesellschaften bestimmen wir den Tarif anhand der geschätzen Anzahl von Unfällen pro Kunde
* Zielsetzung: Wie viele Unfälle werden die potenziellen Kunden haben?

<img src='img/pixabay/accident-151668_1280.png' style="height: 230px">
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Kausales Modell

<img src='img/insurance-causal-model.png' style="height: 300px;">
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Daten entstehen über die Zeit
1. Am Anfang haben wir keine empirischen Daten
1. Wir bewerten Kunden anhand eines einfachen Regelsystems
1. Ground Truth kommt verzögert und kann sich über die Zeit noch verändern
1. Wir sprechen eher von Jahren als Monaten 
</textarea>
</section>

<section data-markdown style="font-size: xx-large;" class="hands-on">
	<textarea data-template>
## Hands-On 1 - Exploration gemeinsam durchgehen

Wir gehen zusammen durch das Notebook

http://localhost:8888/notebooks/notebooks/exploration.ipynb

* Dieser Stand ist das Ende mancher Projekte, bei uns ist es der Anfang
* Was sind die Features?
* Was wollen wir vorhersagen?
* Was für eine Art Modell haben wir?
* Wie gut ist das Modell?

falls es Installationsprobleme gibt: https://colab.research.google.com/github/openknowledge/mlops-data2day/blob/main/notebooks/exploration.ipynb
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Exkursion: Architektur

_Die Summe der wichtigen Entscheidungen_

Was bedeutet dabei wichtig?
- nur schwer änderbar, aber mit einer gewissen Unsicherheit verbunden
- haben das Potential das Projekt zum scheitern zu bringen
- so gewichtig, dass sie Einfluss haben auf das was man sinnvoll bauen kann  
- nur weil man eine Entscheidung nicht bewusst trifft, heißt nicht, dass sie nicht wichtig ist

Diese Definition ist allgemein auch unabhängig von ML gängig
    </textarea>
</section>

<section data-markdown style="font-size: xx-large;" class="hands-on">
	<textarea data-template>
## Hands-On 1b - Exploration nachvollziehen

* Tut euch wieder im Team zusammen
* führt das Notebook Schritt für Schritt aus
* versucht *alle* Unklarheiten zu klären
* falls das im Team nicht geht, fragt gern Olli

_Welche Architektur-Entscheidungen sind hier bereits getroffen worden?_
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Phase I: Architektur-Entscheidungen sammeln

``model-card.md``
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Typische Checks bei Deep Learning

<img src="img/rasbt-deep.png">

https://twitter.com/rasbt/status/1565798671781961728    
  </textarea>
</section>


<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* _Ein lauffähiges ML System_
* Betrieb
* Abschluss / Rückblick

</textarea>
</section>

	<section data-markdown class="fragments">
### Phase I: Exploration

* in der ersten Phase eines Machine Learning Projekts wird die Anwendungsidee validiert und ein
funktionsfähiges Modell entwickelt.
* dabei ist ein schnelles iterieren und ausprobieren von Ideen zentral
* das Ziel ist *nicht* ein sinnvolles Stück Software
* Scripting passt hier besser als Programmieren als Ausdruck für die Tätigkeit
* das Ziel ist eine schnelle Entwicklung
* Phase 1 endet entweder mit
  * einem funktionsfähigen Modell mit dem man in Phase II übergeht oder
  * dem Verwerfen des Ansatzes

	</section>

  <section data-markdown>
	<textarea data-template>
    ### Kann man sinnvoll direkt in Produktion gehen?

<img src="img/ml-workflow/6.PNG" class="fragment">

</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
## Was will man da denn in Produktion bringen?

_ein Modell steht nicht für sich allein_
* es braucht Code für Vor- und Nachbearbeitung
* es ist eingebettet in andere Systeme

_wer will Scripte in Produktion?_
* Notebooks sind interaktive Scripte
* Wie ruft man das auf?
* Welches bringen wir in Produktion?
* Wie versionieren wir das? 
* Tests/Dokumentation?
* Debugging/Show References/Refactor/Autocomplete/Quick Fix/etc.?

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Exploration hinterlässt gern einen Wust an Notebooks

<img src='img/sketch/notebook-explosion-no-title.png'  style="height: 600px;">

</textarea>
</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Ist ein Wust an Notebooks ein Problem?

* das ist kein Zeichen von einem unprofessionellen Vorgehen
* ergibt sich aus der Arbeitsweise und Zielsetzung
* jeder Experimentator, erprobte ML Ansatz und jede Iteration kann eine neue, komplett entkoppelte Kopie eines Notebooks rechtfertigen
  * "Das Wichtigste in dieser Phase ist die schnelle Iteration" https://twitter.com/marktenenholtz/status/1488134981985583105
  * "Machen Sie ein einfaches Experiment nach dem anderen" https://karpathy.github.io/2019/04/25/recipe/
* natürlich wird dabei teilweise falsch entkoppelt
  * Wir kopieren alles, auch die Teile, die die beiden Notebooks größtenteils unverändert teilen
  * Solange wir aber nicht wissen was die relevant gemeinsamen Teile sind müssen wir damit weiter machen
* welcher Ansatz mehr Liebe verdient wird erst am Ende dieser Phase klar
</textarea>
	</section>

	<section data-markdown class="fragments">
### Phase II: Professionalisierung

* in der zweiten Phase wird die skizzierte Lösung in ein langlebiges Projekt umgewandelt
* alle Regeln einer guten Software-Entwicklung gelten von nun an
* Stabilität und Funktionalität wird gewährleistet
* Die Rahmenbedingungen der Produktionsumgebung müssen erfüllt werden
* Art des Deployments, Sprache, Latenz, Speicher, Bandbreite, etc.
* Phase II endet entweder mit
  * reifem Code und Modell mit dem man in Phase III übergeht oder
  * dem Iterieren zurück in Phase I mit neu gewonnenen Erkenntnissen oder falls Rahmenbedingungen nicht erfüllt werden

	</section>


	<section data-markdown>
		<textarea data-template>
<img src='img/sketch/ml-dev-prozess.png' style="height: 650px;">
</textarea>
	</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Was gehört in Bibliotheken ausgelagert?

* Wir können nicht mit einem Notebook in Produktion gehen
  * also muss alles was wir in Produktion brauchen aus den Notebooks herausgezogen werden
* Bestimmte Teile eines Notebooks haben sich als stabil herausgestellt und sollten nicht bei jeder Kopie entkoppelt werden
* Alles was sich nach Software anfühlt (Klassen, Funktionen, etc.) ist auch Software
  * diese Teile sollten auch wie solche behandelt werden  
* Professionalisierung muss oft gut abhängen
  * Es stellt sich erst langsam heraus, was in ein Modul gehört
  * erste Version der extrahierten Module ist mit Sicherheit nicht endgültig
</textarea>
	</section>

<section data-markdown>
### Scripte	

* Manche Notebooks werden in Phase II zu Skripten, die eine dünne API um die Module sind. 
* Können auch in CI/CD eingebaut werden.

</section>

<section data-markdown>
	<textarea data-template>
### CI/CD bei klassischem DevOps

<img src="img/ci-cd-flow-desktop.png" style="height: 100%">

https://www.redhat.com/en/topics/devops/what-is-ci-cd

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### DevOps vs MLOps

<img src="img/devops-vs-mlops.png" style="height: 600px">
</textarea>
</section>
	

<section data-markdown style="font-size: x-large;" class="hands-on">
	<textarea data-template>
## Hands-On II - Scripte, manuelles CI/CD

_Wir führen unsere Pipeline aus und sehen uns gemeinsam die Scripte an_

1. `conda activate mlops-workshop-d2d`
1. `cd scripts`
1. Modell trainieren: `./train.py -d ../data/reference.csv -m classifier`
   * Windows: `python train.py -d ..\data\reference.csv -m classifier`
1. Modell validieren: `./validate.py -d ../data/reference.csv -m classifier`
   * Windows: `python validate.py -d ..\data\reference.csv -m classifier`
1. Deploy: Modell in `app` Ordner schieben

Aufgabe:
1. Sorge dafür, dass die Scripte durchlaufen
1. Passe ggf. die Werte an, sodass alle Scripte durchlaufen
1. Machen die Validierungen für dich Sinn, was könnte sonst noch Sinn machen? 
   * Inspiration: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45742.pdf
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Phase II: Architektur-Entscheidungen sammeln

*Professionalisierung*

``model-card.md``
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Typische Werkzeuge für Workflows und Orchestrierung

* Java, allgemein: https://www.jenkins.io/
* TensorFlow, Tfx: https://www.tensorflow.org/tfx
  * https://www.tensorflow.org/tfx/guide/beam_orchestrator
  * https://www.tensorflow.org/tfx/guide/airflow
  * https://www.tensorflow.org/tfx/guide/kubeflow
* Schießen wie Pilze aus dem Boden

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Große Dateien versionieren

Sinnvoll für Trainingsdaten und Modell 

* git lfs: https://git-lfs.github.com/
* DVC: https://dvc.org/
	</textarea>
</section>


<!-- <section data-markdown class="fragments">
		<textarea data-template>
### Eine ML Lösung hat 2 Artefakte: Code und Modell

* Beides muss in Prod gebracht werden
* Daten und Modell muss extern gehalten, aber zusammen mit Code versioniert werden
* Modell beschreibt einen Ausschnitt der Realität. Wie finde ich heraus wie gut es das tut uns vor allem für relevante Teile der Welt und was mache ich wenn sich das ändert
* Richtung: Benchmark und Monitoring der Lösung in Produktion

</textarea>
	</section> -->

	<section data-markdown>
		<textarea data-template>
### Nicht aller Code ist für Produktion gedacht		

* Was geht in Produktion
  * Vorhersage
  * Monitoring

* Was geht nicht in Produktion
  * Training
  * Analytics
  * Visualisierung
</textarea>
	</section>


	<section data-markdown>
	<textarea data-template>
### Wie kann man ML in Produktion bringen?

<img src="img/mlops/Micro-ML.png">
</textarea>
</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIa - Lokale Produktionsumgebung

1. Im Ordner `app` ist eine Server-Anwendung vorbereitet
1. In `app.py` ist ein einfacher Flask Server implementiert
1. `cd app`
1. Start über `python app.py`
1. Checken, ob der Service läuft: http://localhost:8080/ping
1. Für HTML clients Web Server auf 8000 starten: `python -m http.server 8000`
1. http://localhost:8000/app/client.html macht einen einfachen Requests
1. in `app/client.html` Werte anpassen
   * auf dich als Fahrer
   * außerhalb des gültigen Bereichs des Models 
</textarea>
	</section>

  	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIb - Server Code verstehen

1. `app/app.py` und `app/model_prediction.py` enthalten den Server Code
1. Sieh dir an, wie entschieden wird, ob das ML Modell genommen wird
1. Findest du die Bedingungen sinnvoll?
1. Passe sie deinen Vorstellungen an und probiere das Modell wieder über den Client aus
1. Kannst du dir auch einen Test für diese differenziertere Vorhersage denken? 
</textarea>
	</section>


<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIc - Docker Umgebung

1. `Dockerfile.app` enthält die Bauanleitung für das Docker Image
1. `docker-compose up --build` (Referenz https://docs.docker.com/compose/reference/up/)
   * auf M1 Mac kann der `app`-Service nicht über Docker gestartet werden, daher diesen Abschnitt aus `docker-compose.yml` löschen
   * startet einen Service zum Trainieren unseres Modells
   * baut und startet den Server
   * startet zusätzlich das Monitoring (ignorieren wir erst einmal)
   * Definition in `docker-compose.yaml`
1. `docker-compose down` stopt alle Services wieder   
1. http://localhost:8000/app/client.html kann den über Docker gestarteten Server wieder ansprechen

<small>https://github.com/openknowledge/mlops-data2day#docker
</small>

</textarea>
	</section>

<section data-markdown>
	<textarea data-template>
## Phase II: Architektur-Entscheidungen sammeln

*Betrieb*

``model-card.md``
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauffähiges ML System
* _Betrieb / Monitoring_
* Abschluss / Rückblick

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
<img src='img/everybody-gansta.png' style="height: 600px">

<small>

https://twitter.com/karpathy/status/1486215976559398915
</small>

</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Wir erinnern uns: ML Modelle brauchen permanentes Monitoring und Wartung

<img src='img/verfall-2.PNG'>

</textarea>
</section>

	<section data-markdown class="fragments">
### Phase III: Produktion / Betrieb

in der dritten Phase wird die Lösung in Betrieb genommen

* alle Regeln des produktiven Einsatzes von Software gelten auch hier
* Monitoring hat zusätzliche Herausforderungen
  * Natur und Verteilung der Anfragen und auch Vorhersagen muss permanent überwacht werden
* Phase III endet entweder mit
  * der Abschaltung 
    * entweder bald weil nutzlos oder
    * später weil durch neues System ersetzt
  * dem Iterieren zurück in Phase II mit neu gewonnenen Erkenntnissen
  * dem Iterieren zurück in Phase I mit neu gewonnenen Erkenntnissen oder einem Neuansatz (häufig ebenfalls ein Zeichen für einen Fehlschlag)

			</section>

<section data-markdown class="fragments">
### Grundregel

* Alle haben Probleme in Produktion
* Es gibt kein fehlerfreies System 
  * jedenfalls nicht für lange
* Ziel ist es, Fehler schnell 
  * zu entdecken
  * zu analysieren
  * und entsprechende ihrer Schwere zu adressieren

</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Woher weiß man, dass man ein neues Modell in Produktion braucht?

1. Schon in der Explorationsphase prüfen wie sich das Modell auf neueren Daten verhält
   * wie schnell degradiert die Performance?
   * Mindestens einmal im Jahr, damit man überhaupt noch weiß wie es geht
1. Wenn die Metrik des Modells nachlässt in Produktion
   * Dafür braucht man die Ground Truth der Daten aus Produktion
   * Manchmal bekommt man diese unmittelbar nach der Vorhersage durch die Reaktion eines menschlichen Benutzers
   * Oft aber auch erst nach nennenswerter Verzögerung 
1. Wenn sich die Verteilung der Daten der Anfragen oder Vorhersagen deutlich von denen des Trainings unterscheiden 

</textarea>
	</section>

<section data-markdown>
### Die vier goldenen Regeln des traditionellen Monitorings

* Latency
* Traffic
* Errors
* Saturation

https://sre.google/sre-book/monitoring-distributed-systems/	
</section>

<section data-markdown style="font-size: x-large;">
	<textarea data-template>
### Monitoring für ML muss komplexer sein

<img src="img/google-ml-monitoring.png">

Long: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf
Short: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45742.pdf
</textarea>
</section>

<section data-markdown class="fragments">
## Was kann man machen: Frühes Monitoring

* Qualität der Daten
  * wie verändern sich fehlende oder falsche Felder
  * Plausibilität
* Daten Drift
  * Verteilung der Eingabedaten
* Prediction Drift   
  * Was gibt das Modell aus?
</section>

<section data-markdown>
## Datenqualität

* Felder
  * fehlen
  * ungültig
  * falsch / unplausibel / Wertebereich verlassen
* Features 
  * konstante (die (meisten) Eingaben haben (fast) denselben Wert)
  * leere
  * fast leer
  * Korrelationen zwischen Features
</section>
	
<section data-markdown>
	<textarea data-template>
### Monitoring mit Evidently, Prometheus und Grafana

<!-- <img src="img/mlops/monitoring-components.png"> -->
<img src="img/mlops/evidently_grafana_service.png">

<small>https://evidentlyai.com/blog/evidently-and-grafana-ml-monitoring-live-dashboards
<br>
https://docs.evidentlyai.com/integrations/evidently-and-grafana
</small>

</textarea>
</section>

<section data-markdown>
### Wir setzen einen eigenen Monitoring-Server auf

* Basiert auf Prometheus, Grafana und Evidently
* Code basiert auf https://github.com/evidentlyai/evidently/tree/main/examples/integrations/grafana_monitoring_service
* Nicht direkt Teil des eigentlichen Prod-Servers
* Zusätzliche Requests werden gegen den Monitoring-Server gemacht
* Requests über die Zeit verteilt werden simuliert
</section>
	
<!-- <section data-markdown>
### Prometheus

Metric Server

* https://prometheus.io/
* https://prometheus.io/docs/prometheus/latest/getting_started/
  * https://prometheus.io/download/

</section>

<section data-markdown>
### Grafana

Visualisierung mit Dashboards
	
https://play.grafana.org/
</section>

<section data-markdown>
### Evidently

ML Performance Monitoring
	
https://evidentlyai.com/
</section>
 -->
<section data-markdown>
	<textarea data-template>
### Übersicht der Services

* Monitoring App
  * Endpunkt für die Registrierung von Datensätzen: http://localhost:8085/iterate/<dataset\> (POST)
	* Nutzlast per JSON Body, kann Prediction enthalten z.B.: `[{"speed": 99.0, "age": 28.0, "miles": 21.0, "group": 2.0, "risk": 0.2110376180699707}]`  
  * Metrics aus Evidently: http://localhost:8085/metrics
    * über `app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {"/metrics": prometheus_client.make_wsgi_app()})`

* Promethus: http://localhost:9090
* Grafana: http://localhost:3000

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Drift-Erkennung mit Prometheus, Evidently und Grafana

<img src="img/mlops/grafana-evidently-drift.png">

https://docs.evidentlyai.com/reports/data-drift
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wie erkennen wir Drift?

* Es wird ein Statistischer Test auf den Eingabe-Daten ausgeführt
* Unsere Features sind numerisch und kategorisch
  * in `metrics_app/config.yaml` festgelegt
* Die Anfragen in Production werden verglichen mit Referenz-Datensatz, den wir zum Training benutzt haben (`datasets/insurance`)
* Evidently sucht als Default eine passende Metrik aus, es muss also nicht unser Problem sein
* Man kann aber auch von Hand konfigurieren, sowohl Test als auch Parameter
  * https://docs.evidentlyai.com/user-guide/customization/options-for-statistical-tests
  * https://docs.evidentlyai.com/user-guide/customization/options-for-data-target-drift

https://docs.evidentlyai.com/reports/data-drift

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Welcher Statistischer Test / welche Metrik?

es gibt leider nicht den einen passenden Test

* manche passen nur gut für kleine (< 1000) Datenmengen
  * unsere Datenmengen sind größer als 1000
* manche können nicht nur auf numerischen sondern kategorischen Daten arbeiten
  * wir brauchen beides
* manche sind zwischen 0 und 1 normiert
  * das ist uns eher egal
* unsere Metriken
  * Wasserstein Metrik für numerische Daten
  * Jensen-Shannon Distanz für kategorische Daten  

https://evidentlyai.com/blog/data-drift-detection-large-datasets
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Die Wasserstein-Metrik

_Wenn jede Verteilung als ein Haufen von „Erde“ angehäuft auf dem metrischen Raum betrachtet wird, dann beschreibt diese
Metrik die minimalen „Kosten“ der Umwandlung eines Haufens in den anderen._

* nicht zu sensitiv, zeigt nur größere Veränderungen an
* normiert in Veränderungen in Standardabweichungen
* kann (offensichtlich) über 1 gehen
* ab 0.1 gehen wir von einem Drift aus
* funktioniert nur für numerische Daten

https://de.wikipedia.org/wiki/Wasserstein-Metrik
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Jensen-Shannon Divergenz

* Jensen-Shannon Distanz ist wie Wurzel aus der Divergenz, das ist unsere Metrik
* zwischen 0 und 1
* ab 0.1 gehen wir von einem Drift aus
* funktioniert auch für kategorische Daten
* basiert auf Kullback–Leibler Divergenz, relative Entropie
* Histogramme werden verglichen, Größe des Samples daher egal
* Binning für kategorische Daten offensichtlich
* Intuition: wie viel Information/Entropie/Überraschung steckt im Unterschied der beiden Verteilungen?


https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence
https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
</textarea>
</section>

<!-- <section data-markdown class="fragments">
	<textarea data-template>
### P-Werte

* die statistische Verteilung der jeweiligen Features
* weicht diese im Prod signifikant von der Verteilung im Training ab?
* diese Abweichung wird über eine Metrik berechnet
* es kommt eine Konfidenz heraus, ob die Verteilungen unterschiedlich sind
* ab (default) 95% Konfidenz geht man von einer Abweichung aus 
* das bedeutet, dass es eine 5% Wahrscheinlichkeit gibt, dass die Verteilungen eigentlich doch gleich sind, man es aber nur gerade schlechte Beispiele sieht

https://de.wikipedia.org/wiki/P-Wert
</textarea>
</section> -->

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV - Monitoring und Sinn daraus machen

### Teil I: Das Monitoring starten 

1. Starte die Monitoring Umgebung (das ist wahrscheinlich bereits im vorherigen Schritt passiert) 
   * `docker compose up -d --build`
1. Checke, dass diese Services laufen
   * Monitoring App, Metrics aus Evidently: http://localhost:8085/metrics
   * Prometheus: http://localhost:9090
   * Grafana: http://localhost:3000

</textarea>
	</section>

<section data-markdown>
  <textarea data-template>
### Produktion simulieren

* wir simulieren 3 Jahre Betrieb
* jeder Monat hat 1500 Datensätze
* insgesamt 36 Monate, 54.000 Datensätze
  </textarea>
</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV - Monitoring und Sinn daraus machen

### Teil II: Request simulieren

1. Einloggen in Grafana: http://localhost:3000
   1. admin/admin
1. Das Dashboard `Evidently Data Drift Dashboard` aufrufen
1. Darin den Datensatz `insurance`
1. Mit `python scripts/example_run_request.py` Anfragen simulieren
1. Stelle sicher, dass sich das Dashboard aktualisiert
1. Was beobachtest du? Welche Features driften? Wie kann man das erklären?

</textarea>
	</section>

  <section data-markdown>
  <textarea data-template>
### Manche Features driften

<img src="img/feature-drift.png">
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Was ist passiert?

* die Performance des Modells degradiert wahrscheinlich
* aber wir haben erst nach Jahren eine Ground Truth, die uns das anhand der Metrik zeigt
* als Ersatzmetrik nehmen wir Feature drift
  * miles: extremer Drift
  * age: signifikanter Drift
  * emergency_braking: signifikanter Drift

  </textarea>
</section>

<section data-markdown>
## Alarm bei Data Drift

* Verändert sich die Art der Eingabedaten?
  * Oft ausgedrückt durch die Verteilung der einzelnen Features

* Muss nicht notwendig schlimm sein
  * Wenn entscheidende Features betroffen sind, kann es ein Problem sein 
* Alarm anhand von
  * Wichtigkeit der driftenden Features
  * Anzahl der driftenden Features
  * Ausmaß des Drifts
<!-- * Dafür wichtig
  * Vernünftige Tests anhand der Metriken
  * Vernünftige Konfidenzintervalle -->

</section>

<section data-markdown>
### Drift braucht Interpretation
  
1. Leute werden immer Älter, das passiert aber langsam (age)
1. Es wird immer weniger Auto gefahren, Leute steigen um auf die Bahn und öffentliche Verkehrsmittel (miles)
1. Die Sicherheit der Autos wird immer besser und der Einfluss der individuellen Fahrleistung wird verringert (emergency_braking, pred) 
 
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Wir schummeln leicht
## Wir nutzen die Ground Truth von in einem Jahr     

1. `./validate.py -d ../data/month-12.csv -m classifier`
1. Performance des Modells ist tatsächlich stark zurück gegangen
1. Unsere Interpretation des Drifts war eine gute Ersatzmetrik
</textarea>
</section>



	<!-- <section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV, Monitoring und Sinn daraus machen

### Teil III: Einen passenden Grafana Alert erzeugen

1. Einen Alert kann man in Grafana als Regel oder direkt auf einem Dashboard einrichten
1. Editiere im Dashboard entweder das Panel für die P-Werte oder für den Anteil driftender Features
1. Im Alert Tab kannst die Bedingung für den Alarm festlegen
1. Eventuell musst du erst einen Ordner erzeugen: https://grafana.com/docs/grafana/latest/dashboards/dashboard_folders/
1. Nach dem Speichern sollte ein farbiges Herz auf dem Panel den Status angeben
1. Stelle sicher, dass dein Alert ausgelöst wird
1. Im Alerting Menu können Contact Points zur Benachrichtigung angelegt werden

https://grafana.com/docs/grafana/latest/alerting/

</textarea>
	</section>
 -->

<section data-markdown>
	<textarea data-template>
# Agenda

* Grundlagen MLOps, Unser Beispiel
* Ein lauffähiges ML System
* Betrieb
* _Abschluss / Rückblick_

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Zusammenfassung: Arten von Drift

* _Covariate / Input drift_: Verteilung der Eingabe hat sich geändert
* _Prior / Label drift_: Verteilung der Vorhersage hat sich geändert
* _Concept / Model drift_: Zusammenhang zwischen Eingabe und Vorhersage hat sich geändert

<img src="https://docs.seldon.io/projects/alibi-detect/en/stable/_images/bg_2d_drift.png" style="height: 100%;">

<small>https://docs.seldon.io/projects/alibi-detect/en/stable/cd/background.html#what-is-drift
</small>
</textarea>
</section>

<section data-markdown style="font-size: x-large;">
	<textarea data-template>
## Drift erfordert Interpretation

Wenn die Welt sich ändert, ist Drift zu erwarten und damit ok

|   | Positive Interpretation, keine Maßnahme erforderlich  | Negative Interpretation, Maßnahme erforderlich  |
|---|---|---|
| *Data und Prediction Drift*  | wichtige Features haben sich geändert, Modell kommt klar und extrapoliert gut, z.B.: Höheres Alter, mehr Risiko  |  wichtige Features haben sich geändert, Modell extrapoliert nicht sinnvoll |
| *Data aber kein Prediction Drift*  | keine wichtigen Features geändert, das Modell ist robust genug für den Drift  | wichtige Features geändert, Modell extrapoliert nicht sinnvoll |
| *Prediction aber kein Data Drift*  | ???  | wichtige Features geändert, die Metric hat das aber nicht detektiert |
|   |   |   |

</textarea>
</section>

<section data-markdown>
## Maßnahmen bei Drift

* Neue Version des Modells trainieren
  * Neue Daten aufnehmen (und labeln)
  * Neue Features erzeugen
* Schnelle Maßnahme
  * Pre-/Post-Processing des Modells neu kalibrieren
  * Schwellwerte für Anwendung anpassen
  * Bestimmte Bereiche ausklammern 
  * Modell Architektur ändern (oder fixen) und neu trainieren
* Sehr schnelle Maßnahme: Fallback
  * Manuell
  * Heuristik / Baseline
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wir gehen zurück in die Phasen I und II

* hier wird wieder in Notebooks gearbeitet
* die Bibliotheken werden inkludiert und bei jeder Änderung neu geladen
* Jupyter Lab bietet eine gemeine Oberfläche für Notebooks und Bibliotheken
* Eine Kombination von Visual Studio Code und Jupyer Notebooks ist ebenso möglich
* Rückkehr in Phase I muss nicht radikal sein
  * wenn Ansatz vergleichbar kann Phase II so erhalten bleiben
  * neue Ergebnisse fliesen dann iterativ in die Professionalisierung ein
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Rückkehr in Phase II: Wie trainiert man neu/nach/weiter?

<img src="img/mlops/stateful-vs-stateless.png">

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Stateful vs Stateless

* Stateless (aka offline)
  * trainiert das Modell periodisch komplett neu
  * verhindert "catastrophic forgetting" (https://en.wikipedia.org/wiki/Catastrophic_interference)
* Stateful (aka online)
  * trainiert permanent weiter mit kleinen Batches
  * eine Art des Transfer Learnings
  * folgt Drifts schnell
  * erfordert zeitnah neue Datenpaare 
  * Online Learning for Recommendations at Grubhub: https://arxiv.org/abs/2107.07106
    * angeblich 45x mal weniger aufwändig als offline
    * 20% bessere Metric als offline

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Beispiel für Stateless / Offline Training    

1. Wir trainieren weiterhin mit einem einzelnen Monat
1. Wir könnten auch ältere Daten nehmen, aber durch den Drift scheint der neueste Datensatz am erfolgversprechendsten
1. `./train.py -d ../data/month-12.csv -m classifier`
1. Das Training erfüllt unsere Anforderungen nicht mehr
1. Entweder
   1. Anforderungen anpassen oder
   1. Zurück an den Zeichentisch (Phase I)
      1. Model tunen
      1. Andere Features nehmen?
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Analyse des Drifts für Phase I

http://localhost:8888/notebooks/notebooks/analysis.ipynb

Fallback: https://colab.research.google.com/github/openknowledge/mlops-data2day/blob/main/notebooks/analysis.ipynb
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Was kann man sonst noch machen

* Outlier-Detection
  * Unser Modell wird nicht extrapolieren können
  * Werte außerhalb des Trainings-Bereichs werden wahrscheinlich unrealistisch sicher vorhergesagt
  * Ausreißer müssen ohne Ground Truth entdeckt werden 
* Adversarial Detection
  * Bestimmte Eingaben können absichtlich eine grob falsche Vorhersage herbei führen 
  * Solche Eingaben können erkannt und korrigiert werden
  * Dazu kann z.B. ein Autoencoder benutzt werden, der die Eingabe korrigiert


<small>https://docs.seldon.io/projects/alibi-detect/en/stable/od/methods.html
<br>
https://docs.seldon.io/projects/alibi-detect/en/stable/ad/methods.html
</small>
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Drift Detection für Bilder

* Input Drift mit Histogrammen von Low-Level-Features (HSV): https://towardsdatascience.com/detecting-semantic-drift-within-image-data-6a59a0e768c6
* Input Drift mit Dimensions-Reduktion: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_ks_cifar10.html 
* Model Drift durch Vergleich mit destilliertem Modell: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_distillation_cifar10.html 
* Ein komplettes Beispiel mit Alibi-Detect: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/alibi_detect_deploy.html#4.-Drift-detection-with-Kolmogorov-Smirnov
* Anomalien in Bildern: https://29a.ch/photo-forensics
* Drift kann auch auf Text festgestellt werden: https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_text_imdb.html
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Wann liegt Ground Truth vor?

*Menschliche Experten können die Ground Truth*
* *bestimmen* 
  * sobald ein Mensch die Entscheidungen nachprüft / revidiert
  * bei einem Proposal-System kann das sehr schnell sein
  * bei Dunkelverarbeitung sollte dies in regelmäßigen Abständen passieren
* *nicht bestimmen*
  * wir müssen Realitäten abwarten
  * bei Zeitreihen wird auf die nahe Zukunft vorhergesagt, sobald diese Eintritt kann überprüft werden
  * oft sind solche Realitäten erst nach wahrnehmbar und unterliegen statistischen Schwankungen (wie bei uns)
</textarea>
</section>


<section data-markdown class="fragments">
<textarea data-template>
### Zusammenfassung

1. Machine Learning Projekte können in Phasen gedacht werden
1. In der ersten Phase macht man möglichst schnelle Experimente
1. Sollte sich eine Idee als tragfähig erweisen, professionalisiert man die Idee
1. Dies ist Voraussetzung und Grundlage für Produktion
1. In Produktion ergeben sich besondere Herausforderung im Bereich Monitoring
1. Typischerweise müssen Machine Learning Systeme regelmäßig nachtrainiert und gepflegt werden
</textarea>
</section>

		<section data-markdown>
			<textarea data-template>
# Vielen Dank

MLOps mit Python

Bleibt gern im Kontakt

Oliver Zeigermann / oliver.zeigermann@openknowledge.de
https://www.linkedin.com/in/oliver-zeigermann-34989773/
https://twitter.com/DJCordhose

Tobias Kurzydym / tobias.kurzydym@openknowledge.de
https://twitter.com/tkurzydym
</textarea>
		</section>

		</div>
	</div>
	<script src="revealjs/reveal.js/dist/reveal.js"></script>
	<script src="revealjs/reveal.js/plugin/notes/notes.js"></script>
	<script src="revealjs/reveal.js/plugin/markdown/markdown.js"></script>
	<script src="revealjs/reveal.js/plugin/highlight/highlight.js"></script>
	<script src="revealjs/config.js"></script>


</body>

</html>