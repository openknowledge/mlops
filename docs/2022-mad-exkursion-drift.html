<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=10.0, user-scalable=yes">

	<title>MLOps</title>

	<link rel="stylesheet" href="revealjs/reveal.js/dist/reset.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/reveal.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/theme/white.css" />

    <!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/monokai.css"> -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/zenburn.css"> -->
    <link rel="stylesheet" href="revealjs/highlight-js-github-theme.css" />
    <link rel="stylesheet" href="revealjs/styles.css" />

</head>

<body style="background-color: whitesmoke;">
	<div class="reveal">
		<div class="slides">

<section data-markdown>
    <textarea data-template>
# Exkursion: Drift

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Wir erinnern uns: ML Modelle brauchen permanentes Monitoring und Wartung

<img src='img/verfall-2.PNG'>

</textarea>
</section>


	<section data-markdown class="fragments">
		<textarea data-template>
### Woher weiß man, dass man ein neues Modell in Produktion braucht?

1. Schon in der Explorationsphase prüfen, wie sich das Modell auf neueren Daten verhält
   * Wie schnell degradiert die Performance?
   * Mindestens einmal im Jahr, damit man überhaupt noch weiß, wie es geht
1. Wenn die Metrik des Modells in Produktion nachlässt
   * Dafür braucht man die Ground Truth der Daten aus Produktion
   * Manchmal bekommt man diese unmittelbar nach der Vorhersage durch die Reaktion eines menschlichen Benutzers
   * Oft aber auch erst nach nennenswerter Verzögerung 
1. *Wenn sich die Verteilung der Daten der Anfragen oder Vorhersagen deutlich von denen des Trainings unterscheiden* 

</textarea>
	</section>

<section data-markdown>
  <textarea data-template>
### Verteilung?

* Verteilungen
  * die wichtigste: Normalverteilung aka Gauß-Verteilung
* Histogramme (Binning)

<img src="img/nobel.svg">

</textarea>
</section>
  
  
  <section data-markdown class="fragments">
    <textarea data-template>
  ### Verteilungen
  
  <img src="img/causal-insurance/age-reference.png">
  
  Anhand des Alters der Versicherten zum Zeitpunkt des Trainings
  </textarea>
  </section>
  

<section data-markdown class="fragments">
  <textarea data-template>
### Zum Zeitpunkt des Trainings?

<img src="img/drift_entschlackt.png" style="height: 100%;">

</textarea>
</section>

<!-- <section data-markdown class="fragments">
  <textarea data-template>
### So sieht die Verteilung jetzt aus

<img src="img/causal-insurance/age-current.png">

</textarea>
</section>
 -->
<section data-markdown class="fragments">
	<textarea data-template>
### Driftet das?

<img src="img/causal-insurance/age_no_drift_p75.png">

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Und das?

<img src="img/causal-insurance/age_drift_p0.png">

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wie erkennen wir Drift?

* Es wird ein statistischer Test auf den Verteilungen ausgeführt
* Die Anfragen in Produktion werden verglichen mit Referenz-Datensatz, den wir zum Training benutzt haben

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Welcher statistischer Test / welche Metrik?

es gibt leider nicht den einen passenden Test

* manche passen nur gut für kleine (< 1000) Datenmengen
  * unsere Datenmengen sind größer als 1000
* manche können nicht nur auf numerischen, sondern kategorischen Daten arbeiten
  * wir brauchen beides
* manche sind zwischen 0 und 1 normiert
  * das ist uns eher egal
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
### Speziell für Drift-Erkennung in Frage kommende Tests

* Kolmogorov-Smirnov-Test
* Population Stability Index
* Kullback-Leibler-Divergenz
* Jensen-Shannon-Distanz  
* Wasserstein-Distanz
  
https://evidentlyai.com/blog/data-drift-detection-large-datasets
</textarea>
</section>

<section data-markdown  class="fragments">
	<textarea data-template>
### Kolmogorov-Smirnov-Test

* numerisch
* wird gerne als Default gewählt, kann bei großen Datenmengen aber zu empfindlich sein
* Vorteil: keine Normalverteilung vorausgesetzt
* ein 'typischer' statistischer Test mit p als Rückgabewert
* Nullhypothese: die beiden Verteilungen sind gleich
* Drift bei p-Wert unter 0.05

*Wann nutzen?* Wenn Drift schnell entdeckt werden muss oder die Datenmengen kleiner ausfallen

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### P-Werte

* die statistische Verteilung der jeweiligen Features
* weicht diese im Prod signifikant von der Verteilung im Training ab? (Nullhypothese: nein)
* diese Abweichung wird über eine Metrik berechnet
* es kommt eine Konfidenz heraus, ob die Nullhypothese stimmt
* unter 5% Konfidenz geht man von einer Abweichung aus <span style="font-size: 0.95rem">(1% oder 0.1% auch möglich)</span>
* das bedeutet, dass es eine 5% Wahrscheinlichkeit gibt, dass die Verteilungen eigentlich doch gleich sind, man es aber nur gerade schlechte Beispiele sieht

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### P-Werte für unsere beiden potentiellen Drifts

<div class="container">
<div class="col">
  <img src="img/causal-insurance/age_no_drift_p75.png" style="height: 100%;">
  <em>p-Wert (Wahrscheinlichkeit der Nullhypothese) = 75%, also kein Drift</em>
</div>
<div class="col">
  <img src="img/causal-insurance/age_drift_p0.png" style="height: 100%;">
  <em>p-Wert (Wahrscheinlichkeit der Nullhypothese) < 0,1%, also Drift</em>
  
</div>
</div>

</textarea>
</section>

<section data-markdown class="fragments">
## Maßnahmen bei Drift

* *Neue Version des Modells trainieren*
  * Neue Daten aufnehmen (und labeln)
  * Neue Features erzeugen
  * Modell Architektur ändern (oder fixen) und neu trainieren
* Schnelle Maßnahme
  * Pre-/Post-Processing des Modells neu kalibrieren
  * Schwellwerte für Anwendung anpassen
  * Bestimmte Bereiche ausklammern 
* Sehr schnelle Maßnahme: Fallback
  * Manuell
  * Heuristik / Baseline
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Was kann man sonst noch machen

* Outlier-Detection
  * Unser Modell wird nicht extrapolieren können
  * Werte außerhalb des Trainings-Bereichs werden wahrscheinlich unrealistisch sicher vorhergesagt
  * Ausreißer müssen ohne Ground Truth entdeckt werden 
* Adversarial Detection
  * Bestimmte Eingaben können absichtlich eine grob falsche Vorhersage herbei führen 
  * Solche Eingaben können erkannt und korrigiert werden
  * Dazu kann z.B. ein Autoencoder benutzt werden, der die Eingabe korrigiert


<small>https://docs.seldon.io/projects/alibi-detect/en/stable/od/methods.html
<br>
https://docs.seldon.io/projects/alibi-detect/en/stable/ad/methods.html
</small>
</textarea>
</section>

		</div>
	</div>
	<script src="revealjs/reveal.js/dist/reveal.js"></script>
	<script src="revealjs/reveal.js/plugin/notes/notes.js"></script>
	<script src="revealjs/reveal.js/plugin/markdown/markdown.js"></script>
	<script src="revealjs/reveal.js/plugin/highlight/highlight.js"></script>
	<script src="revealjs/config.js"></script>


</body>

</html>